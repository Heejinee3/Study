<br />

## LangGraph란?

LangChain 기반의 상태 기반(Stateful) AI 애플리케이션을 설계하고 실행할 수 있게 해주는 프레임워크

> LangGraph = LangChain + 상태 흐름 제어 (Stateful Graph Execution)

<br />

## 설치

```python
pip install langgraph
pip install langchain
pip install langchain-openai
pip install python-dotenv
```

<br />

## Chatbot 예제

1. State 정의 (State)
   messages: 이전 대화 메시지를 저장하는 리스트
2. Node 정의 (generate)
   입력된 메시지를 OpenAI 모델에 보내고 응답을 받아옴
3. 환경 변수 로딩
   .env 파일에서 API 키를 불러옴
4. 모델 설정
   ChatOpenAI(model="gpt-4o-mini") 사용
5. 그래프 생성 및 연결
   노드 "generate"를 만든 후, START → generate → END 흐름으로 구성
6. MemorySaver
   이전 대화 상태를 저장합니다 (세션 유지용)
7. 메인 루프
   사용자가 입력하면 graph.stream()을 통해 모델 응답을 실시간으로 받고 출력
   exit, quit, q 입력 시 종료

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from dotenv import load_dotenv

from typing import Annotated
from typing_extensions import TypedDict

# 상태 (State)
class State(TypedDict):
    # messages 필드는 누적되는 메시지 리스트
    messages: Annotated[list[str], add_messages]

# 노드 (Node)
def generate(state: State):
    return {"messages": [model.invoke(state["messages"])]}

# 환경 변수에서 API 키 불러오기
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)

# OpenAI 모델 설정 (gpt-4o-mini 사용)
model = ChatOpenAI(model="gpt-4o-mini")

graph_builder = StateGraph(State) # Graph Builder 생성
graph_builder.add_node("generate", generate) # 'generate' Node 추가
graph_builder.add_edge(START, "generate") # 'START → generate' Edge 추가
graph_builder.add_edge("generate", END) # 'generate → END' Edge 추가

memory = MemorySaver() # 메모리 체크포인트 저장소 생성
config = {"configurable": {"thread_id": "abcd"}} # 사용자별로 구분할 수 있는 thread_id 설정

# 그래프 컴파일
graph = graph_builder.compile(checkpointer=memory)

while True:
    user_input = input("You\t:")

    if user_input in ["exit", "quit", "q"]:
        break

    for event in graph.stream({
        "messages": [HumanMessage(user_input)]},
        config,
        stream_mode="values"
    ):
        event["messages"][-1].pretty_print()

```

<details>
    <summary>TypedDict</summary>

- 딕셔너리에 타입 힌트를 줄 수 있게 해주는 기능
- TypedDict를 사용하면, 딕셔너리 구조를 클래스처럼 정의
- 런타임(runtime)에는 타입 검사를 하지 않음
- 타입 검사 도구(mypy, pyright)도 제대로 체크

```python
from typing_extensions import TypedDict

class User(TypedDict):
    name: str
    age: int

def print_user(user: User):
    print(f"{user['name']} is {user['age']} years old")
```

</details>

<details>
    <summary>Annotated</summary>

- 파이썬 타입 힌트에 "추가 정보(메타데이터)"를 붙일 수 있게 해주는 도구

```python
from typing import Annotated

x: Annotated[int, "양수여야 함"]
```

</details>

<details>
    <summary>stream_mode</summary>

**stream_mode=”messages”**

- 상태 객체 중 messages 키의 값을 추적해서 출력 (대화 기록 중심)
- 채팅 대화 내용을 실시간 출력

```python
[
	...,
	(
		AIMessageChunk(
			content='녕하세요',
			additional_kwargs={},
			response_metadata={},
			id='run--3bfec6a7-023f-4410-8f0b-4fdbb83e664d'
		),
		{
			'langgraph_step': 1,
			'langgraph_node': 'generate',
			'langgraph_triggers': ('branch:to:generate',),
			'langgraph_path': ('__pregel_pull', 'generate'),
			'langgraph_checkpoint_ns': 'generate:4075bc20-5572-7a4a-60b2-89e2ced9b47c',
			'checkpoint_ns': 'generate:4075bc20-5572-7a4a-60b2-89e2ced9b47c',
			'ls_provider': 'openai',
			'ls_model_name': 'gpt-4o-mini',
			'ls_model_type': 'chat',
			'ls_temperature': None
		}
	),
	...
]
```

<p></p>

**stream_mode=”values”**

- 각 노드에서 return한 값을 추적해서 출력 (출력값 중심)
- 노드가 생성한 출력값을 추적하고 싶을 때 (분기 처리, 디버깅)

```python
[
	{
		'messages': [
			HumanMessage(
				content='날씨가 참 좋아~',
				additional_kwargs={},
				response_metadata={},
				id='e005b93f-5585-4f94-9b87-c4416f63ba39'
			)
		]
	},
	{
		'messages': [
			HumanMessage(
				content='날씨가 참 좋아~',
				additional_kwargs={},
				response_metadata={},
				id='e005b93f-5585-4f94-9b87-c4416f63ba39'
			),
			AIMessage(
				content='정말 좋죠! 맑은 날씨에는 기분이 한층 더 상쾌해지는 것 같아요. 이런 날은 산책이나 야외 활동을 하기에 좋겠네요. 특 별한 계획이 있으신가요?',
				additional_kwargs={'refusal': None},
				response_metadata={
					'token_usage': {
						'completion_tokens': 52,
						'prompt_tokens': 13,
						'total_tokens': 65,
						'completion_tokens_details': {
							'accepted_prediction_tokens': 0,
							'audio_tokens': 0,
							'reasoning_tokens': 0,
							'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
							'audio_tokens': 0,
							'cached_tokens': 0
						}
					},
					'model_name': 'gpt-4o-mini-2024-07-18',
					'system_fingerprint': None,
					'id': 'chatcmpl-BuD3tOwVOnOwArlziD3ZQyJ09FGZq',
					'service_tier': 'default',
					'finish_reason': 'stop',
					'logprobs': None
				},
				id='run--6e1085e1-834e-4fb7-82c9-b2e6ed7e5c6f-0',
				usage_metadata={
					'input_tokens': 13,
					'output_tokens': 52,
					'total_tokens': 65,
					'input_token_details': {
						'audio': 0,
						'cache_read': 0
					},
					'output_token_details': {
						'audio': 0,
						'reasoning': 0
					}
				}
			)
		]
	},
	...
]
```

<p></p>

| stream_mode | description                                          |
| ----------- | ---------------------------------------------------- |
| messages    | 메시지를 스트림 방식으로 실시간 출력                 |
| values      | LangGraph의 단계별로 상태 변화 추적                  |
| updates     | 단계별로 변경된 내용만 반환                          |
| debug       | 디버깅용 옵션으로 실행되는 과정의 정보를 자세히 제공 |
| custom      | 사용자 정의 방식으로 스트림을 설정                   |

</details>

<br />
<br />
<br />

[출처: 이성용, 「Do it! LLM을 활용한 AI 에이전트 개발 입문 - GPT API+딥시크+라마+랭체인+랭그래프+RAG」, 이지스퍼블리싱](https://www.easyspub.co.kr/20_Menu/BookView/764/PUB)

<br />
