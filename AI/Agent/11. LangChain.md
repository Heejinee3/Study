<br />

## LangChain이란?

- 언어 모델(LLM, Large Language Model)을 중심으로 다양한 기능을 쉽게 구현할 수 있게 도와주는 Python (또는 JavaScript) 기반 프레임워크
- LLM을 실제 응용 프로그램에 쉽게 연결하고 활용하게 해주는 도구 모음

<br />

## 설치

```json
pip install langchain
pip install langchain-openai
```

<br />

## 기본

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# OpenAI API Key 등록
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)

# 모델 초기화
llm = ChatOpenAI(model="gpt-4o")

# 초기 시스템 메시지
messages = [
    SystemMessage("너는 사용자를 도와주는 상담사야."),
]

while True:
    user_input = input("사용자: ") # 사용자 입력 받기

    if user_input == "exit": # 사용자가 대화를 종료하려는지 확인
        break

    messages.append(HumanMessage(user_input)) # 사용자 메시지를 대화 기록에 추가하기

    ai_response = llm.invoke(messages) # 대화 기록을 기반으로 AI 응답 가져오기

    messages.append(ai_response) # AI 응답 대화 기록에 추가하기

    print("AI: " + ai_response.content) # AI 응답 출력

```

<br />

## Message History

- 사용자와 AI 사이의 대화 기록을 저장하고 관리하는 기능
- LLM은 기본적으로 이전 대화를 기억하지 못하기 때문에 LangChain은 Memory라는 구조를 통해 대화를 저장하고, 그 기록을 매번 프롬프트에 자동으로 포함시켜 줌

<p></p>

- 예시

  ```python
  from dotenv import load_dotenv
  from langchain_openai import ChatOpenAI
  from langchain_core.messages import HumanMessage, SystemMessage
  from langchain_core.chat_history import InMemoryChatMessageHistory
  from langchain_core.runnables.history import RunnableWithMessageHistory

  # OpenAI API Key 등록
  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  # 모델 초기화
  llm = ChatOpenAI(model="gpt-4o")

  # 세션별 대화 기록을 저장할 딕셔너리
  store = {}

  # 세션 ID에 따라 대화 기록을 가져오는 함수
  def get_session_history(session_id: str):
      if session_id not in store:
          store[session_id] = InMemoryChatMessageHistory()
          store[session_id].add_message(SystemMessage("너는 사용자를 도와주는 상담사야."))
      return store[session_id]

  # 모델 실행 시 대화 기록을 함께 전달하는 래퍼 객체 생성
  with_message_history = RunnableWithMessageHistory(llm, get_session_history)

  # 세션 ID를 설정하는 config 객체 생성
  config = {"configurable": {"session_id": "a72f3e1c-984a-4b5e-a9e2-85cf5c12a8b3"}}

  while True:
      user_input = input("사용자: ") # 사용자 입력 받기

      if user_input == "exit": # 사용자가 대화를 종료하려는지 확인
          break

      ai_response = with_message_history.invoke([HumanMessage(user_input)], config=config) # 대화 기록을 기반으로 AI 응답 가져오기

      print("AI: " + ai_response.content) # AI 응답 출력
  ```

<br />

## LCEL (LangChain Expression Language)

LangChain에서 체인을 마치 파이프라인처럼 연결해서 쓸 수 있게 해주는 문법

<br />

---

### Ex1. StrOutputParser

- LLM의 응답을 그냥 문자열(string) 그대로 반환할 때 사용
- 기존에는 체인을 이런 식으로 만들었을 때,

  ```python
  result = model.invoke(messages) # result = AIMessage(content='안녕하세요, 개스톤! 당신의 초대는 정말 감사하지만, 저는 당신과 함께 저녁을 먹는 것에 대해 조금 망설여져요. 저는 더 특별한 인연을 찾고 있답니다. 어떻게 생각하세요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 62, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-1c36b7f0-a1c2-41b4-abeb-702fb4512d9c-0', usage_metadata={'input_tokens': 62, 'output_tokens': 54, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
  result = parser.invoke(result) # result = '안녕하세요, 개스톤! 당신의 초대는 감사하지만, 저는 다른 일들이 있어서 저녁 약속은 어렵겠어요. 하지만, 좋은 친구와 함께 시간을 보내는 것도 즐겁지 않을까요? 당신과 함께하는 저녁은 즐거울 것 같네요. 어떤 계획이 있나요?
  ```

  LCEL을 쓰면 훨씬 간결하고 선언적으로 표현

  ```python
  chain = model | parser
  result = chain.invoke(messages) # result = '안녕하세요, 개스톤! 당신의 초대는 감사하지만, 저는 다른 일들이 있어서 저녁 약속은 어렵겠어요. 하지만, 좋은 친구와 함께 시간을 보내는 것도 즐겁지 않을까요? 당신과 함께하는 저녁은 즐거울 것 같네요. 어떤 계획이 있나요?'
  ```

<p></p>

- 예시

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from langchain_core.messages import HumanMessage, SystemMessage
  from dotenv import load_dotenv

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  model = ChatOpenAI(model="gpt-4o-mini")

  messages = [
      SystemMessage(content="너는 미녀와 야수에 나오는 미녀야. 그 캐릭터에 맞게 사용자와 대화하라."),
      HumanMessage(content="안녕? 저는 개스톤입니다. 오늘 시간 괜찮으시면 저녁 같이 먹을까요?"),
  ]

  parser = StrOutputParser()

  chain = model | parser
  result = chain.invoke(messages)
  ```

<br />

---

### Ex2. ChatPromptTemplate

- LLM에 보낼 메시지 형식(prompt)을 쉽게 만들고 재사용할 수 있도록 도와주는 템플릿 도구

  ```python
  system_template = "너는 {story}에 나오는 {character_a} 역할이다. 그 캐릭터에 맞게 사용자와 대화하라."
  human_template = "안녕? 저는 {character_b}입니다. 오늘 시간 괜찮으시면 {activity} 같이 할까요?"

  prompt_template = ChatPromptTemplate([
      ("system", system_template),
      ("user", human_template),
  ])

  # result = [SystemMessage(content='너는 미녀와 야수에 나오는 미녀 역할이다. 그 캐릭터에 맞게 사용자와 대화하라.', additional_kwargs={}, response_metadata={}), HumanMessage(content='안녕? 저는 야수입니다. 오늘 시간 괜찮으시면 저녁 같이 할까요?', additional_kwargs={}, response_metadata={})]
  result = prompt_template.invoke({
      "story": "미녀와 야수",
      "character_a": "미녀",
      "character_b": "야수",
      "activity": "저녁"
  })
  ```

<p></p>

- 예시

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from dotenv import load_dotenv
  from langchain_core.prompts import ChatPromptTemplate

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  system_template = "너는 {story}에 나오는 {character_a} 역할이다. 그 캐릭터에 맞게 사용자와 대화하라."
  human_template = "안녕? 저는 {character_b}입니다. 오늘 시간 괜찮으시면 {activity} 같이 할까요?"

  prompt_template = ChatPromptTemplate([
      ("system", system_template),
      ("user", human_template),
  ])

  model = ChatOpenAI(model="gpt-4o-mini")

  parser = StrOutputParser()

  chain = prompt_template | model | parser
  result = chain.invoke({
      "story": "미녀와 야수",
      "character_a": "미녀",
      "character_b": "개스톤",
      "activity": "저녁"
  })
  ```

<br />

---

### Ex3. Structured Output

- LLM의 응답을 단순한 텍스트가 아니라, 지정한 데이터 구조(Pydantic 모델)에 맞춰 파싱

<p></p>

- 예시

  ```python
  from dotenv import load_dotenv
  from langchain_openai import ChatOpenAI
  from langchain_core.prompts import ChatPromptTemplate
  from pydantic import BaseModel, Field
  from typing import Literal

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  system_template = "너는 {story}에 나오는 {character_a} 역할이다. 그 캐릭터에 맞게 사용자와 대화하라."
  human_template = "안녕? 저는 {character_b}입니다. 오늘 시간 괜찮으시면 {activity} 같이 할까요?"

  prompt_template = ChatPromptTemplate.from_messages([
      ("system", system_template),
      ("human", human_template),
  ])

  base_model = ChatOpenAI(model="gpt-4o-mini")

  class Adlib(BaseModel):
      """스토리 설정과 사용자 입력에 반응하는 대사를 만드는 클래스"""
      answer: str = Field(description="스토리 설정과 사용자와의 대화 기록에 따라 생성된 대사")
      main_emotion: Literal["기쁨", "분노", "슬픔", "공포", "냉소", "불쾌", "중립"] = Field(description="대사의 주요 감정")
      main_emotion_intensity: float = Field(description="대사의 주요 감정의 강도 (0.0 ~ 1.0)")

  structured_model = base_model.with_structured_output(Adlib)

  chain = prompt_template | structured_model

  result = chain.invoke({
      "story": "미녀와 야수",
      "character_a": "벨",
      "character_b": "개스톤",
      "activity": "저녁"
  })

  ```

<br />

## @tool Decorator

- 해당 함수를 LangChain Tool로 등록해서 LLM이 사용할 수 있도록 만들어주는 데코레이터
- @tool을 붙이면 LangChain 내부에서 이 함수가 사용 가능한 도구(tool)로 인식

<p></p>

- 예시

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.messages import HumanMessage, SystemMessage
  from langchain_core.tools import tool
  from datetime import datetime
  from dotenv import load_dotenv
  import pytz

  @tool # @tool 데코레이터를 사용하여 함수를 도구로 등록
  def get_current_time(timezone: str, location: str) -> str:
      """ 현재 시각을 반환하는 함수

      Args:
          timezone (str): 타임존 (예: 'Asia/Seoul') 실제 존재하는 타임존이어야 함
          location (str): 지역명. 타임존이 모든 지명에 대응되지 않기 때문에 이후 llm 답변 생성에 사용됨
      """
      tz = pytz.timezone(timezone)
      now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
      location_and_local_time = f'{timezone} ({location}) 현재시각 {now} '
      return location_and_local_time

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  llm = ChatOpenAI(model="gpt-4o-mini")

  # 도구 리스트와 도구 이름-함수 매핑 딕셔너리 생성
  tools = [get_current_time,]
  tool_dict = {"get_current_time": get_current_time,}

  # LLM에 도구들을 연결(bind)해서 도구 호출 가능하도록 확장
  llm_with_tools = llm.bind_tools(tools)

  messages = [
      SystemMessage("너는 사용자의 질문에 답변을 하기 위해 tools를 사용할 수 있다."),
      HumanMessage("부산은 지금 몇시야?"),
  ]

  # 확장된 LLM을 사용해 메시지 기반 답변 생성
  response = llm_with_tools.invoke(messages) # (1)
  messages.append(response)

  # LLM 응답에 포함된 도구 호출 지시(함수 호출 명령)들을 순회
  for tool_call in response.tool_calls:
      selected_tool = tool_dict[tool_call["name"]]
      tool_msg = selected_tool.invoke(tool_call) # (2)
      messages.append(tool_msg)

  # 도구 결과가 포함된 메시지 히스토리를 다시 LLM에 보내 후속 대화 생성
  final_response = llm_with_tools.invoke(messages) # (3)
  ```

<details>
    <summary> 변수값</summary>

```python
##### (1) #####
AIMessage(
		content='',
		additional_kwargs={
				'tool_calls': [
						{
								'id': 'call_xC0QhLkdBKHhihHG4zHFRiBX',
								'function': {
										'arguments': '{"timezone":"Asia/Seoul","location":"Busan"}',
										'name': 'get_current_time'
								},
								'type': 'function'
						}
				],
				'refusal': None
		},
		response_metadata={
				'token_usage': {
						'completion_tokens': 24,
						'prompt_tokens': 135,
						'total_tokens': 159,
						'completion_tokens_details': {
								'accepted_prediction_tokens': 0,
								'audio_tokens': 0,
								'reasoning_tokens': 0,
								'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
								'audio_tokens': 0,
								'cached_tokens': 0
						}
				},
				'model_name': 'gpt-4o-mini-2024-07-18',
				'system_fingerprint': 'fp_00428b782a',
				'finish_reason': 'tool_calls',
				'logprobs': None
		},
		id='run-eed19018-9933-4516-9241-f6c5a32a6366-0',
		tool_calls=[
				{
						'name': 'get_current_time',
						'args': {
								'timezone': 'Asia/Seoul',
								'location': 'Busan'
						},
						'id': 'call_xC0QhLkdBKHhihHG4zHFRiBX',
						'type': 'tool_call'
				}
		],
		usage_metadata={
				'input_tokens': 135,
				'output_tokens': 24,
				'total_tokens': 159,
				'input_token_details': {
						'audio': 0,
						'cache_read': 0
				},
				'output_token_details': {
						'audio': 0,
						'reasoning': 0
				}
		}
)
```

```python
##### (2) #####
ToolMessage(
		content='Asia/Seoul (Busan) 현재시각 2025-02-18 00:38:38 ',
		name='get_current_time',
		tool_call_id='call_xC0QhLkdBKHhihHG4zHFRiBX'
)
```

```python
##### (3) #####
AIMessage(
		content='현재 부산은 2025년 2월 18일 00시 38분 38초입니다.',
		additional_kwargs={'refusal': None},
		response_metadata={
				'token_usage': {
						'completion_tokens': 26,
						'prompt_tokens': 192,
						'total_tokens': 218,
						'completion_tokens_details': {
								'accepted_prediction_tokens': 0,
								'audio_tokens': 0,
								'reasoning_tokens': 0,
								'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
								'audio_tokens': 0,
								'cached_tokens': 0
						}
				},
				'model_name': 'gpt-4o-mini-2024-07-18',
				'system_fingerprint': 'fp_00428b782a',
				'finish_reason': 'stop',
				'logprobs': None
		},
		id='run-c66dd490-141a-4aea-8749-02a3fcbbebcb-0',
		usage_metadata={
				'input_tokens': 192,
				'output_tokens': 26,
				'total_tokens': 218,
				'input_token_details': {
						'audio': 0,
						'cache_read': 0
				},
				'output_token_details': {
						'audio': 0,
						'reasoning': 0
				}
		}
)
```

</details>

<p></p>

- 파이단틱(Pydantic)을 사용하여 매개변수의 형식을 명확하게 정의 가능

  ```python
  from pydantic import BaseModel, Field

  class StockHistoryInput(BaseModel):
      ticker: str = Field(..., title="주식 코드", description="주식 코드 (예: AAPL)")
      period: str = Field(..., title="기간", description="주식 데이터 조회 기간 (예: 1d, 1mo, 1y)")

  @tool
  def get_yf_stock_history(stock_history_input: StockHistoryInput) -> str:
      """ 주식 종목의 가격 데이터를 조회하는 함수"""
      stock = yf.Ticker(stock_history_input.ticker)
      history = stock.history(period=stock_history_input.period)
      history_md = history.to_markdown()

      return history_md
  ```

<br />

## Stream 방식으로 출력하는 Chatbot 예시

```python
import streamlit as st
from dotenv import load_dotenv
from datetime import datetime
import pytz

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool

# 도구 함수 정의
# 예시)
# time = get_current_time("Asia/Seoul", "서울")
# time = "Asia/Seoul (서울) 현재시각 2025-07-04 18:00:00"
@tool
def get_current_time(timezone: str, location: str) -> str:
    """현재 시각을 반환하는 함수."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) 현재시각 {now}'
        return result
    except pytz.UnknownTimeZoneError:
        return f"알 수 없는 타임존: {timezone}"

# 사용자의 메시지 처리하기 위한 함수
def get_ai_response(messages):
    response = llm_with_tools.stream(messages)

    gathered = None
    for chunk in response:
        yield chunk

        if gathered is None:
            gathered = chunk
        else:
            gathered += chunk

    if gathered.tool_calls:
        st.session_state.messages.append(gathered)

        for tool_call in gathered.tool_calls:
            selected_tool = tool_dict[tool_call['name']]
            tool_msg = selected_tool.invoke(tool_call)
            st.session_state.messages.append(tool_msg)

        for chunk in get_ai_response(st.session_state.messages):
            yield chunk

# OpenAI API Key 등록
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)

# 모델 초기화
llm = ChatOpenAI(model="gpt-4o-mini")

# 도구 바인딩
tools = [get_current_time]
tool_dict = {"get_current_time": get_current_time}

llm_with_tools = llm.bind_tools(tools)

# Streamlit 앱
st.title("💬 Chat")

# 스트림릿 session_state에 메시지 저장
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("너는 사용자를 돕기 위해 최선을 다하는 인공지능 봇이다. "),
        AIMessage("How can I help you?")
    ]

# 스트림릿 화면에 메시지 출력
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, ToolMessage):
            st.chat_message("tool").write(msg.content)

# 사용자 입력 처리
if prompt := st.chat_input():
    st.chat_message("user").write(prompt) # 사용자 메시지 출력
    st.session_state.messages.append(HumanMessage(prompt)) # 사용자 메시지 저장

    response = get_ai_response(st.session_state["messages"])

    result = st.chat_message("assistant").write_stream(response) # AI 메시지 출력

    st.session_state.messages.append(AIMessage(result)) # AI 메시지 저장

```

<details>
    <summary>변수값</summary>
    get_ai_response 함수 안에 있는 chunk와 gathered의 변수값은 다음과 같음

<details>
    <summary>function calling을 하지 않을 때</summary>

- messages

  ```
  messages = [
      SystemMessage("너는 사용자를 돕기 위해 최선을 다하는 인공지능 봇이다. "),
      AIMessage("How can I help you?")
      HumanMessage("안녕!")
  ]
  ```

- chunk

  ```python
  ##### chunk(1) #####
  {
  		"content":""
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ##### chunk(2) #####
  {
  		"content":"안"
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ##### chunk(3) #####
  {
  		"content":"녕하세요"
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ...

  ##### chunk(n-1) #####
  {
  		"content":"?"
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ##### chunk(n) #####
  {
  		"content":""
  		"additional_kwargs":{}
  		"response_metadata":{
  				"finish_reason":"stop"
  				"model_name":"gpt-4o-mini-2024-07-18"
  				"system_fingerprint":"fp_34a54ae93c"
  				"service_tier":"default"
  		}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }
  ```

- gathered

  ```python
    {
    		"content":"안녕하세요! 어떻게 도와드릴까요?"
    		"additional_kwargs":{}
    		"response_metadata":{
    				"finish_reason":"stop"
    				"model_name":"gpt-4o-mini-2024-07-18"
    				"system_fingerprint":"fp_34a54ae93c"
    				"service_tier":"default"
    		}
    		"type":"AIMessageChunk"
    		"name":NULL
    		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
    		"example":false
    		"tool_calls":[]
    		"invalid_tool_calls":[]
    		"usage_metadata":NULL
    		"tool_call_chunks":[]
    }
  ```

</details>

<details>
    <summary>function calling을 할 때</summary>

- messages

  ```python
  messages = [
      SystemMessage("너는 사용자를 돕기 위해 최선을 다하는 인공지능 봇이다. "),
      AIMessage("How can I help you?")
      HumanMessage("대전 시간은 몇시야?")
  ]
  ```

- chunk

  ```python
  ##### chunk(1) #####
  {
  		"content":""
  		"additional_kwargs":{
  				"tool_calls":[
  						0:{
  								"index":0
  								"id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
  								"function":{
  										"arguments":""
  										"name":"get_current_time"
  								}
  								"type":"function"
  						}
  				]
  		}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[
  				0:{
  						"name":"get_current_time"
  						"args":{}
  						"id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
  						"type":"tool_call"
  				}
  		]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[
  				0:{
  						"name":"get_current_time"
  						"args":""
  						"id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
  						"index":0
  						"type":"tool_call_chunk"
  				}
  		]
  }

  ##### chunk(2) #####
  {
  		"content":""
  		"additional_kwargs":{
  				"tool_calls":[
  						0:{
  								"index":0
  								"id":NULL
  								"function":{
  										"arguments":"{""
  										"name":NULL
  								}
  								"type":NULL
  						}
  				]
  		}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[
  				0:{
  						**"name":""**
  						"args":{}
  						**"id":NULL**
  						"type":"tool_call"
  				}
  		]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[
  				0:{
  						**"name":NULL**
  						**"args":"{""**
  						**"id":NULL**
  						"index":0
  						"type":"tool_call_chunk"
  				}
  		]
  }

  ...

  ##### chunk(n-1) #####
  {
  		"content":""
  		"additional_kwargs":{
  				"tool_calls":[
  						0:{
  								"index":0
  								"id":NULL
  								"function":{
  										"arguments":""}"
  										"name":NULL
  								}
  								"type":NULL
  						}
  				]
  		}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[
  				0:{
  						"name":NULL
  						"args":""}"
  						"id":NULL
  						"error":NULL
  						"type":"invalid_tool_call"
  				}
  		]
  		"usage_metadata":NULL
  		"tool_call_chunks":[
  				0:{
  						"name":NULL
  						"args":""}"
  						"id":NULL
  						"index":0
  						"type":"tool_call_chunk"
  				}
  		]
  }

  ##### chunk(n) #####
  {
  		"content":""
  		"additional_kwargs":{}
  		"response_metadata":{
  				"finish_reason":"tool_calls"
  				"model_name":"gpt-4o-mini-2024-07-18"
  				"system_fingerprint":"fp_34a54ae93c"
  				"service_tier":"default"
  		}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }
  ```

- gathered

  ```python
      {
              "content":""
              "additional_kwargs":{
                      "tool_calls":[
                              0:{
                                      "index":0
                                      "id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
                                      "function":{
                                              "arguments":"{"timezone":"Asia/Seoul","location":"Daejeon"}"
                                              "name":"get_current_time"
                                      }
                                      "type":"function"
                              }
                      ]
              }
              "response_metadata":{
                      "finish_reason":"tool_calls"
                      "model_name":"gpt-4o-mini-2024-07-18"
                      "system_fingerprint":"fp_34a54ae93c"
                      "service_tier":"default"
              }
              "type":"AIMessageChunk"
              "name":NULL
              "id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
              "example":false
              "tool_calls":[
                      0:{
                              "name":"get_current_time"
                              "args":{
                                      "timezone":"Asia/Seoul"
                                      "location":"Daejeon"
                              }
                              "id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
                              "type":"tool_call"
                      }
              ]
              "invalid_tool_calls":[]
              "usage_metadata":NULL
              "tool_call_chunks":[
                      0:{
                              "name":"get_current_time"
                              "args":"{"timezone":"Asia/Seoul","location":"Daejeon"}"
                              "id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
                              "index":0
                              "type":"tool_call_chunk"
                      }
              ]
      }
  ```

</details>
</details>

<br />
<br />
<br />

[출처: 이성용, 「Do it! LLM을 활용한 AI 에이전트 개발 입문 - GPT API+딥시크+라마+랭체인+랭그래프+RAG」, 이지스퍼블리싱](https://www.easyspub.co.kr/20_Menu/BookView/764/PUB)

<br />
