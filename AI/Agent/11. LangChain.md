<br />

## LangChain이란?

- 언어 모델(LLM, Large Language Model)을 중심으로 다양한 기능을 쉽게 구현할 수 있게 도와주는 Python (또는 JavaScript) 기반 프레임워크
- LLM을 실제 응용 프로그램에 쉽게 연결하고 활용하게 해주는 도구 모음

<br />

## 설치

```json
pip install langchain
pip install langchain-openai
```

<br />

## 기본

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# OpenAI API Key 등록
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)

# 모델 초기화
llm = ChatOpenAI(model="gpt-4o")

# 초기 시스템 메시지
messages = [
    SystemMessage("너는 사용자를 도와주는 상담사야."),
]

while True:
    user_input = input("사용자: ") # 사용자 입력 받기

    if user_input == "exit": # 사용자가 대화를 종료하려는지 확인
        break

    messages.append(HumanMessage(user_input)) # 사용자 메시지를 대화 기록에 추가하기

    ai_response = llm.invoke(messages) # 대화 기록을 기반으로 AI 응답 가져오기

    messages.append(ai_response) # AI 응답 대화 기록에 추가하기

    print("AI: " + ai_response.content) # AI 응답 출력

```

<br />

## Message History

- 사용자와 AI 사이의 대화 기록을 저장하고 관리하는 기능
- LLM은 기본적으로 이전 대화를 기억하지 못하기 때문에 LangChain은 Memory라는 구조를 통해 대화를 저장하고, 그 기록을 매번 프롬프트에 자동으로 포함시켜 줌

<p></p>

- 예시

  ```python
  from dotenv import load_dotenv
  from langchain_openai import ChatOpenAI
  from langchain_core.messages import HumanMessage, SystemMessage
  from langchain_core.chat_history import InMemoryChatMessageHistory
  from langchain_core.runnables.history import RunnableWithMessageHistory

  # OpenAI API Key 등록
  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  # 모델 초기화
  llm = ChatOpenAI(model="gpt-4o")

  # 세션별 대화 기록을 저장할 딕셔너리
  store = {}

  # 세션 ID에 따라 대화 기록을 가져오는 함수
  def get_session_history(session_id: str):
      if session_id not in store:
          store[session_id] = InMemoryChatMessageHistory()
          store[session_id].add_message(SystemMessage("너는 사용자를 도와주는 상담사야."))
      return store[session_id]

  # 모델 실행 시 대화 기록을 함께 전달하는 래퍼 객체 생성
  with_message_history = RunnableWithMessageHistory(llm, get_session_history)

  # 세션 ID를 설정하는 config 객체 생성
  config = {"configurable": {"session_id": "a72f3e1c-984a-4b5e-a9e2-85cf5c12a8b3"}}

  while True:
      user_input = input("사용자: ") # 사용자 입력 받기

      if user_input == "exit": # 사용자가 대화를 종료하려는지 확인
          break

      ai_response = with_message_history.invoke([HumanMessage(user_input)], config=config) # 대화 기록을 기반으로 AI 응답 가져오기

      print("AI: " + ai_response.content) # AI 응답 출력
  ```

<br />

## LCEL (LangChain Expression Language)

LangChain에서 체인을 마치 파이프라인처럼 연결해서 쓸 수 있게 해주는 문법

<br />

---

### Ex1. StrOutputParser

- LLM의 응답을 그냥 문자열(string) 그대로 반환할 때 사용
- 기존에는 체인을 이런 식으로 만들었을 때,

  ```python
  result = model.invoke(messages) # result = AIMessage(content='안녕하세요, 개스톤! 당신의 초대는 정말 감사하지만, 저는 당신과 함께 저녁을 먹는 것에 대해 조금 망설여져요. 저는 더 특별한 인연을 찾고 있답니다. 어떻게 생각하세요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 62, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-1c36b7f0-a1c2-41b4-abeb-702fb4512d9c-0', usage_metadata={'input_tokens': 62, 'output_tokens': 54, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
  result = parser.invoke(result) # result = '안녕하세요, 개스톤! 당신의 초대는 감사하지만, 저는 다른 일들이 있어서 저녁 약속은 어렵겠어요. 하지만, 좋은 친구와 함께 시간을 보내는 것도 즐겁지 않을까요? 당신과 함께하는 저녁은 즐거울 것 같네요. 어떤 계획이 있나요?
  ```

  LCEL을 쓰면 훨씬 간결하고 선언적으로 표현

  ```python
  chain = model | parser
  result = chain.invoke(messages) # result = '안녕하세요, 개스톤! 당신의 초대는 감사하지만, 저는 다른 일들이 있어서 저녁 약속은 어렵겠어요. 하지만, 좋은 친구와 함께 시간을 보내는 것도 즐겁지 않을까요? 당신과 함께하는 저녁은 즐거울 것 같네요. 어떤 계획이 있나요?'
  ```

<p></p>

- 예시

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from langchain_core.messages import HumanMessage, SystemMessage
  from dotenv import load_dotenv

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  model = ChatOpenAI(model="gpt-4o-mini")

  messages = [
      SystemMessage(content="너는 미녀와 야수에 나오는 미녀야. 그 캐릭터에 맞게 사용자와 대화하라."),
      HumanMessage(content="안녕? 저는 개스톤입니다. 오늘 시간 괜찮으시면 저녁 같이 먹을까요?"),
  ]

  parser = StrOutputParser()

  chain = model | parser
  result = chain.invoke(messages)
  ```

<br />

---

### Ex2. ChatPromptTemplate

- LLM에 보낼 메시지 형식(prompt)을 쉽게 만들고 재사용할 수 있도록 도와주는 템플릿 도구

  ```python
  system_template = "너는 {story}에 나오는 {character_a} 역할이다. 그 캐릭터에 맞게 사용자와 대화하라."
  human_template = "안녕? 저는 {character_b}입니다. 오늘 시간 괜찮으시면 {activity} 같이 할까요?"

  prompt_template = ChatPromptTemplate([
      ("system", system_template),
      ("user", human_template),
  ])

  # result = [SystemMessage(content='너는 미녀와 야수에 나오는 미녀 역할이다. 그 캐릭터에 맞게 사용자와 대화하라.', additional_kwargs={}, response_metadata={}), HumanMessage(content='안녕? 저는 야수입니다. 오늘 시간 괜찮으시면 저녁 같이 할까요?', additional_kwargs={}, response_metadata={})]
  result = prompt_template.invoke({
      "story": "미녀와 야수",
      "character_a": "미녀",
      "character_b": "야수",
      "activity": "저녁"
  })
  ```

<p></p>

- 예시

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from dotenv import load_dotenv
  from langchain_core.prompts import ChatPromptTemplate

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  system_template = "너는 {story}에 나오는 {character_a} 역할이다. 그 캐릭터에 맞게 사용자와 대화하라."
  human_template = "안녕? 저는 {character_b}입니다. 오늘 시간 괜찮으시면 {activity} 같이 할까요?"

  prompt_template = ChatPromptTemplate([
      ("system", system_template),
      ("user", human_template),
  ])

  model = ChatOpenAI(model="gpt-4o-mini")

  parser = StrOutputParser()

  chain = prompt_template | model | parser
  result = chain.invoke({
      "story": "미녀와 야수",
      "character_a": "미녀",
      "character_b": "개스톤",
      "activity": "저녁"
  })
  ```

<br />

---

### Ex3. Structured Output

- LLM의 응답을 단순한 텍스트가 아니라, 지정한 데이터 구조(Pydantic 모델)에 맞춰 파싱

<p></p>

- 예시

  ```python
  from dotenv import load_dotenv
  from langchain_openai import ChatOpenAI
  from langchain_core.prompts import ChatPromptTemplate
  from pydantic import BaseModel, Field
  from typing import Literal

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  system_template = "너는 {story}에 나오는 {character_a} 역할이다. 그 캐릭터에 맞게 사용자와 대화하라."
  human_template = "안녕? 저는 {character_b}입니다. 오늘 시간 괜찮으시면 {activity} 같이 할까요?"

  prompt_template = ChatPromptTemplate.from_messages([
      ("system", system_template),
      ("human", human_template),
  ])

  base_model = ChatOpenAI(model="gpt-4o-mini")

  class Adlib(BaseModel):
      """스토리 설정과 사용자 입력에 반응하는 대사를 만드는 클래스"""
      answer: str = Field(description="스토리 설정과 사용자와의 대화 기록에 따라 생성된 대사")
      main_emotion: Literal["기쁨", "분노", "슬픔", "공포", "냉소", "불쾌", "중립"] = Field(description="대사의 주요 감정")
      main_emotion_intensity: float = Field(description="대사의 주요 감정의 강도 (0.0 ~ 1.0)")

  structured_model = base_model.with_structured_output(Adlib)

  chain = prompt_template | structured_model

  result = chain.invoke({
      "story": "미녀와 야수",
      "character_a": "벨",
      "character_b": "개스톤",
      "activity": "저녁"
  })

  ```

<br />

## @tool Decorator

- 해당 함수를 LangChain Tool로 등록해서 LLM이 사용할 수 있도록 만들어주는 데코레이터
- @tool을 붙이면 LangChain 내부에서 이 함수가 사용 가능한 도구(tool)로 인식

<p></p>

- 예시

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.messages import HumanMessage, SystemMessage
  from langchain_core.tools import tool
  from datetime import datetime
  from dotenv import load_dotenv
  import pytz

  @tool # @tool 데코레이터를 사용하여 함수를 도구로 등록
  def get_current_time(timezone: str, location: str) -> str:
      """ 현재 시각을 반환하는 함수

      Args:
          timezone (str): 타임존 (예: 'Asia/Seoul') 실제 존재하는 타임존이어야 함
          location (str): 지역명. 타임존이 모든 지명에 대응되지 않기 때문에 이후 llm 답변 생성에 사용됨
      """
      tz = pytz.timezone(timezone)
      now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
      location_and_local_time = f'{timezone} ({location}) 현재시각 {now} '
      return location_and_local_time

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  llm = ChatOpenAI(model="gpt-4o-mini")

  # 도구 리스트와 도구 이름-함수 매핑 딕셔너리 생성
  tools = [get_current_time,]
  tool_dict = {"get_current_time": get_current_time,}

  # LLM에 도구들을 연결(bind)해서 도구 호출 가능하도록 확장
  llm_with_tools = llm.bind_tools(tools)

  messages = [
      SystemMessage("너는 사용자의 질문에 답변을 하기 위해 tools를 사용할 수 있다."),
      HumanMessage("부산은 지금 몇시야?"),
  ]

  # 확장된 LLM을 사용해 메시지 기반 답변 생성
  response = llm_with_tools.invoke(messages) # (1)
  messages.append(response)

  # LLM 응답에 포함된 도구 호출 지시(함수 호출 명령)들을 순회
  for tool_call in response.tool_calls:
      selected_tool = tool_dict[tool_call["name"]]
      tool_msg = selected_tool.invoke(tool_call) # (2)
      messages.append(tool_msg)

  # 도구 결과가 포함된 메시지 히스토리를 다시 LLM에 보내 후속 대화 생성
  final_response = llm_with_tools.invoke(messages) # (3)
  ```

<details>
    <summary> 변수값</summary>

```python
##### (1) #####
AIMessage(
		content='',
		additional_kwargs={
				'tool_calls': [
						{
								'id': 'call_xC0QhLkdBKHhihHG4zHFRiBX',
								'function': {
										'arguments': '{"timezone":"Asia/Seoul","location":"Busan"}',
										'name': 'get_current_time'
								},
								'type': 'function'
						}
				],
				'refusal': None
		},
		response_metadata={
				'token_usage': {
						'completion_tokens': 24,
						'prompt_tokens': 135,
						'total_tokens': 159,
						'completion_tokens_details': {
								'accepted_prediction_tokens': 0,
								'audio_tokens': 0,
								'reasoning_tokens': 0,
								'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
								'audio_tokens': 0,
								'cached_tokens': 0
						}
				},
				'model_name': 'gpt-4o-mini-2024-07-18',
				'system_fingerprint': 'fp_00428b782a',
				'finish_reason': 'tool_calls',
				'logprobs': None
		},
		id='run-eed19018-9933-4516-9241-f6c5a32a6366-0',
		tool_calls=[
				{
						'name': 'get_current_time',
						'args': {
								'timezone': 'Asia/Seoul',
								'location': 'Busan'
						},
						'id': 'call_xC0QhLkdBKHhihHG4zHFRiBX',
						'type': 'tool_call'
				}
		],
		usage_metadata={
				'input_tokens': 135,
				'output_tokens': 24,
				'total_tokens': 159,
				'input_token_details': {
						'audio': 0,
						'cache_read': 0
				},
				'output_token_details': {
						'audio': 0,
						'reasoning': 0
				}
		}
)
```

```python
##### (2) #####
ToolMessage(
		content='Asia/Seoul (Busan) 현재시각 2025-02-18 00:38:38 ',
		name='get_current_time',
		tool_call_id='call_xC0QhLkdBKHhihHG4zHFRiBX'
)
```

```python
##### (3) #####
AIMessage(
		content='현재 부산은 2025년 2월 18일 00시 38분 38초입니다.',
		additional_kwargs={'refusal': None},
		response_metadata={
				'token_usage': {
						'completion_tokens': 26,
						'prompt_tokens': 192,
						'total_tokens': 218,
						'completion_tokens_details': {
								'accepted_prediction_tokens': 0,
								'audio_tokens': 0,
								'reasoning_tokens': 0,
								'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
								'audio_tokens': 0,
								'cached_tokens': 0
						}
				},
				'model_name': 'gpt-4o-mini-2024-07-18',
				'system_fingerprint': 'fp_00428b782a',
				'finish_reason': 'stop',
				'logprobs': None
		},
		id='run-c66dd490-141a-4aea-8749-02a3fcbbebcb-0',
		usage_metadata={
				'input_tokens': 192,
				'output_tokens': 26,
				'total_tokens': 218,
				'input_token_details': {
						'audio': 0,
						'cache_read': 0
				},
				'output_token_details': {
						'audio': 0,
						'reasoning': 0
				}
		}
)
```

</details>

<p></p>

- 파이단틱(Pydantic)을 사용하여 매개변수의 형식을 명확하게 정의 가능

  ```python
  from pydantic import BaseModel, Field

  class StockHistoryInput(BaseModel):
      ticker: str = Field(..., title="주식 코드", description="주식 코드 (예: AAPL)")
      period: str = Field(..., title="기간", description="주식 데이터 조회 기간 (예: 1d, 1mo, 1y)")

  @tool
  def get_yf_stock_history(stock_history_input: StockHistoryInput) -> str:
      """ 주식 종목의 가격 데이터를 조회하는 함수"""
      stock = yf.Ticker(stock_history_input.ticker)
      history = stock.history(period=stock_history_input.period)
      history_md = history.to_markdown()

      return history_md
  ```

<br />
<br />
<br />

[출처: 이성용, 「Do it! LLM을 활용한 AI 에이전트 개발 입문 - GPT API+딥시크+라마+랭체인+랭그래프+RAG」, 이지스퍼블리싱](https://www.easyspub.co.kr/20_Menu/BookView/764/PUB)

<br />
