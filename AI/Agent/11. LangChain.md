<br />

## LangChainì´ë€?

- ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” Python (ë˜ëŠ” JavaScript) ê¸°ë°˜ í”„ë ˆì„ì›Œí¬
- LLMì„ ì‹¤ì œ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì‰½ê²Œ ì—°ê²°í•˜ê³  í™œìš©í•˜ê²Œ í•´ì£¼ëŠ” ë„êµ¬ ëª¨ìŒ

<br />

## ì„¤ì¹˜

```json
pip install langchain
pip install langchain-openai
```

<br />

## ê¸°ë³¸

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# OpenAI API Key ë“±ë¡
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o")

# ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
messages = [
    SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."),
]

while True:
    user_input = input("ì‚¬ìš©ì: ") # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°

    if user_input == "exit": # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸
        break

    messages.append(HumanMessage(user_input)) # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°

    ai_response = llm.invoke(messages) # ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°

    messages.append(ai_response) # AI ì‘ë‹µ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°

    print("AI: " + ai_response.content) # AI ì‘ë‹µ ì¶œë ¥

```

<br />

## Message History

- ì‚¬ìš©ìì™€ AI ì‚¬ì´ì˜ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥
- LLMì€ ê¸°ë³¸ì ìœ¼ë¡œ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— LangChainì€ Memoryë¼ëŠ” êµ¬ì¡°ë¥¼ í†µí•´ ëŒ€í™”ë¥¼ ì €ì¥í•˜ê³ , ê·¸ ê¸°ë¡ì„ ë§¤ë²ˆ í”„ë¡¬í”„íŠ¸ì— ìë™ìœ¼ë¡œ í¬í•¨ì‹œì¼œ ì¤Œ

<p></p>

- ì˜ˆì‹œ

  ```python
  from dotenv import load_dotenv
  from langchain_openai import ChatOpenAI
  from langchain_core.messages import HumanMessage, SystemMessage
  from langchain_core.chat_history import InMemoryChatMessageHistory
  from langchain_core.runnables.history import RunnableWithMessageHistory

  # OpenAI API Key ë“±ë¡
  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  # ëª¨ë¸ ì´ˆê¸°í™”
  llm = ChatOpenAI(model="gpt-4o")

  # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
  store = {}

  # ì„¸ì…˜ IDì— ë”°ë¼ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
  def get_session_history(session_id: str):
      if session_id not in store:
          store[session_id] = InMemoryChatMessageHistory()
          store[session_id].add_message(SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."))
      return store[session_id]

  # ëª¨ë¸ ì‹¤í–‰ ì‹œ ëŒ€í™” ê¸°ë¡ì„ í•¨ê»˜ ì „ë‹¬í•˜ëŠ” ë˜í¼ ê°ì²´ ìƒì„±
  with_message_history = RunnableWithMessageHistory(llm, get_session_history)

  # ì„¸ì…˜ IDë¥¼ ì„¤ì •í•˜ëŠ” config ê°ì²´ ìƒì„±
  config = {"configurable": {"session_id": "a72f3e1c-984a-4b5e-a9e2-85cf5c12a8b3"}}

  while True:
      user_input = input("ì‚¬ìš©ì: ") # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°

      if user_input == "exit": # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸
          break

      ai_response = with_message_history.invoke([HumanMessage(user_input)], config=config) # ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°

      print("AI: " + ai_response.content) # AI ì‘ë‹µ ì¶œë ¥
  ```

<br />

## LCEL (LangChain Expression Language)

LangChainì—ì„œ ì²´ì¸ì„ ë§ˆì¹˜ íŒŒì´í”„ë¼ì¸ì²˜ëŸ¼ ì—°ê²°í•´ì„œ ì“¸ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¬¸ë²•

<br />

---

### Ex1. StrOutputParser

- LLMì˜ ì‘ë‹µì„ ê·¸ëƒ¥ ë¬¸ìì—´(string) ê·¸ëŒ€ë¡œ ë°˜í™˜í•  ë•Œ ì‚¬ìš©
- ê¸°ì¡´ì—ëŠ” ì²´ì¸ì„ ì´ëŸ° ì‹ìœ¼ë¡œ ë§Œë“¤ì—ˆì„ ë•Œ,

  ```python
  result = model.invoke(messages) # result = AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ê°œìŠ¤í†¤! ë‹¹ì‹ ì˜ ì´ˆëŒ€ëŠ” ì •ë§ ê°ì‚¬í•˜ì§€ë§Œ, ì €ëŠ” ë‹¹ì‹ ê³¼ í•¨ê»˜ ì €ë…ì„ ë¨¹ëŠ” ê²ƒì— ëŒ€í•´ ì¡°ê¸ˆ ë§ì„¤ì—¬ì ¸ìš”. ì €ëŠ” ë” íŠ¹ë³„í•œ ì¸ì—°ì„ ì°¾ê³  ìˆë‹µë‹ˆë‹¤. ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 62, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-1c36b7f0-a1c2-41b4-abeb-702fb4512d9c-0', usage_metadata={'input_tokens': 62, 'output_tokens': 54, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
  result = parser.invoke(result) # result = 'ì•ˆë…•í•˜ì„¸ìš”, ê°œìŠ¤í†¤! ë‹¹ì‹ ì˜ ì´ˆëŒ€ëŠ” ê°ì‚¬í•˜ì§€ë§Œ, ì €ëŠ” ë‹¤ë¥¸ ì¼ë“¤ì´ ìˆì–´ì„œ ì €ë… ì•½ì†ì€ ì–´ë µê² ì–´ìš”. í•˜ì§€ë§Œ, ì¢‹ì€ ì¹œêµ¬ì™€ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê²ƒë„ ì¦ê²ì§€ ì•Šì„ê¹Œìš”? ë‹¹ì‹ ê³¼ í•¨ê»˜í•˜ëŠ” ì €ë…ì€ ì¦ê±°ìš¸ ê²ƒ ê°™ë„¤ìš”. ì–´ë–¤ ê³„íšì´ ìˆë‚˜ìš”?
  ```

  LCELì„ ì“°ë©´ í›¨ì”¬ ê°„ê²°í•˜ê³  ì„ ì–¸ì ìœ¼ë¡œ í‘œí˜„

  ```python
  chain = model | parser
  result = chain.invoke(messages) # result = 'ì•ˆë…•í•˜ì„¸ìš”, ê°œìŠ¤í†¤! ë‹¹ì‹ ì˜ ì´ˆëŒ€ëŠ” ê°ì‚¬í•˜ì§€ë§Œ, ì €ëŠ” ë‹¤ë¥¸ ì¼ë“¤ì´ ìˆì–´ì„œ ì €ë… ì•½ì†ì€ ì–´ë µê² ì–´ìš”. í•˜ì§€ë§Œ, ì¢‹ì€ ì¹œêµ¬ì™€ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê²ƒë„ ì¦ê²ì§€ ì•Šì„ê¹Œìš”? ë‹¹ì‹ ê³¼ í•¨ê»˜í•˜ëŠ” ì €ë…ì€ ì¦ê±°ìš¸ ê²ƒ ê°™ë„¤ìš”. ì–´ë–¤ ê³„íšì´ ìˆë‚˜ìš”?'
  ```

<p></p>

- ì˜ˆì‹œ

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from langchain_core.messages import HumanMessage, SystemMessage
  from dotenv import load_dotenv

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  model = ChatOpenAI(model="gpt-4o-mini")

  messages = [
      SystemMessage(content="ë„ˆëŠ” ë¯¸ë…€ì™€ ì•¼ìˆ˜ì— ë‚˜ì˜¤ëŠ” ë¯¸ë…€ì•¼. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼."),
      HumanMessage(content="ì•ˆë…•? ì €ëŠ” ê°œìŠ¤í†¤ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ ì €ë… ê°™ì´ ë¨¹ì„ê¹Œìš”?"),
  ]

  parser = StrOutputParser()

  chain = model | parser
  result = chain.invoke(messages)
  ```

<br />

---

### Ex2. ChatPromptTemplate

- LLMì— ë³´ë‚¼ ë©”ì‹œì§€ í˜•ì‹(prompt)ì„ ì‰½ê²Œ ë§Œë“¤ê³  ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í…œí”Œë¦¿ ë„êµ¬

  ```python
  system_template = "ë„ˆëŠ” {story}ì— ë‚˜ì˜¤ëŠ” {character_a} ì—­í• ì´ë‹¤. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼."
  human_template = "ì•ˆë…•? ì €ëŠ” {character_b}ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ {activity} ê°™ì´ í• ê¹Œìš”?"

  prompt_template = ChatPromptTemplate([
      ("system", system_template),
      ("user", human_template),
  ])

  # result = [SystemMessage(content='ë„ˆëŠ” ë¯¸ë…€ì™€ ì•¼ìˆ˜ì— ë‚˜ì˜¤ëŠ” ë¯¸ë…€ ì—­í• ì´ë‹¤. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì•ˆë…•? ì €ëŠ” ì•¼ìˆ˜ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ ì €ë… ê°™ì´ í• ê¹Œìš”?', additional_kwargs={}, response_metadata={})]
  result = prompt_template.invoke({
      "story": "ë¯¸ë…€ì™€ ì•¼ìˆ˜",
      "character_a": "ë¯¸ë…€",
      "character_b": "ì•¼ìˆ˜",
      "activity": "ì €ë…"
  })
  ```

<p></p>

- ì˜ˆì‹œ

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from dotenv import load_dotenv
  from langchain_core.prompts import ChatPromptTemplate

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  system_template = "ë„ˆëŠ” {story}ì— ë‚˜ì˜¤ëŠ” {character_a} ì—­í• ì´ë‹¤. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼."
  human_template = "ì•ˆë…•? ì €ëŠ” {character_b}ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ {activity} ê°™ì´ í• ê¹Œìš”?"

  prompt_template = ChatPromptTemplate([
      ("system", system_template),
      ("user", human_template),
  ])

  model = ChatOpenAI(model="gpt-4o-mini")

  parser = StrOutputParser()

  chain = prompt_template | model | parser
  result = chain.invoke({
      "story": "ë¯¸ë…€ì™€ ì•¼ìˆ˜",
      "character_a": "ë¯¸ë…€",
      "character_b": "ê°œìŠ¤í†¤",
      "activity": "ì €ë…"
  })
  ```

<br />

---

### Ex3. Structured Output

- LLMì˜ ì‘ë‹µì„ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼, ì§€ì •í•œ ë°ì´í„° êµ¬ì¡°(Pydantic ëª¨ë¸)ì— ë§ì¶° íŒŒì‹±

<p></p>

- ì˜ˆì‹œ

  ```python
  from dotenv import load_dotenv
  from langchain_openai import ChatOpenAI
  from langchain_core.prompts import ChatPromptTemplate
  from pydantic import BaseModel, Field
  from typing import Literal

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  system_template = "ë„ˆëŠ” {story}ì— ë‚˜ì˜¤ëŠ” {character_a} ì—­í• ì´ë‹¤. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼."
  human_template = "ì•ˆë…•? ì €ëŠ” {character_b}ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ {activity} ê°™ì´ í• ê¹Œìš”?"

  prompt_template = ChatPromptTemplate.from_messages([
      ("system", system_template),
      ("human", human_template),
  ])

  base_model = ChatOpenAI(model="gpt-4o-mini")

  class Adlib(BaseModel):
      """ìŠ¤í† ë¦¬ ì„¤ì •ê³¼ ì‚¬ìš©ì ì…ë ¥ì— ë°˜ì‘í•˜ëŠ” ëŒ€ì‚¬ë¥¼ ë§Œë“œëŠ” í´ë˜ìŠ¤"""
      answer: str = Field(description="ìŠ¤í† ë¦¬ ì„¤ì •ê³¼ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì— ë”°ë¼ ìƒì„±ëœ ëŒ€ì‚¬")
      main_emotion: Literal["ê¸°ì¨", "ë¶„ë…¸", "ìŠ¬í””", "ê³µí¬", "ëƒ‰ì†Œ", "ë¶ˆì¾Œ", "ì¤‘ë¦½"] = Field(description="ëŒ€ì‚¬ì˜ ì£¼ìš” ê°ì •")
      main_emotion_intensity: float = Field(description="ëŒ€ì‚¬ì˜ ì£¼ìš” ê°ì •ì˜ ê°•ë„ (0.0 ~ 1.0)")

  structured_model = base_model.with_structured_output(Adlib)

  chain = prompt_template | structured_model

  result = chain.invoke({
      "story": "ë¯¸ë…€ì™€ ì•¼ìˆ˜",
      "character_a": "ë²¨",
      "character_b": "ê°œìŠ¤í†¤",
      "activity": "ì €ë…"
  })

  ```

<br />

## @tool Decorator

- í•´ë‹¹ í•¨ìˆ˜ë¥¼ LangChain Toolë¡œ ë“±ë¡í•´ì„œ LLMì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì£¼ëŠ” ë°ì½”ë ˆì´í„°
- @toolì„ ë¶™ì´ë©´ LangChain ë‚´ë¶€ì—ì„œ ì´ í•¨ìˆ˜ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬(tool)ë¡œ ì¸ì‹

<p></p>

- ì˜ˆì‹œ

  ```python
  from langchain_openai import ChatOpenAI
  from langchain_core.messages import HumanMessage, SystemMessage
  from langchain_core.tools import tool
  from datetime import datetime
  from dotenv import load_dotenv
  import pytz

  @tool # @tool ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë“±ë¡
  def get_current_time(timezone: str, location: str) -> str:
      """ í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

      Args:
          timezone (str): íƒ€ì„ì¡´ (ì˜ˆ: 'Asia/Seoul') ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íƒ€ì„ì¡´ì´ì–´ì•¼ í•¨
          location (str): ì§€ì—­ëª…. íƒ€ì„ì¡´ì´ ëª¨ë“  ì§€ëª…ì— ëŒ€ì‘ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´í›„ llm ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ë¨
      """
      tz = pytz.timezone(timezone)
      now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
      location_and_local_time = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now} '
      return location_and_local_time

  env_path = "api_key.env"
  load_dotenv(dotenv_path=env_path)

  llm = ChatOpenAI(model="gpt-4o-mini")

  # ë„êµ¬ ë¦¬ìŠ¤íŠ¸ì™€ ë„êµ¬ ì´ë¦„-í•¨ìˆ˜ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
  tools = [get_current_time,]
  tool_dict = {"get_current_time": get_current_time,}

  # LLMì— ë„êµ¬ë“¤ì„ ì—°ê²°(bind)í•´ì„œ ë„êµ¬ í˜¸ì¶œ ê°€ëŠ¥í•˜ë„ë¡ í™•ì¥
  llm_with_tools = llm.bind_tools(tools)

  messages = [
      SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ê¸° ìœ„í•´ toolsë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤."),
      HumanMessage("ë¶€ì‚°ì€ ì§€ê¸ˆ ëª‡ì‹œì•¼?"),
  ]

  # í™•ì¥ëœ LLMì„ ì‚¬ìš©í•´ ë©”ì‹œì§€ ê¸°ë°˜ ë‹µë³€ ìƒì„±
  response = llm_with_tools.invoke(messages) # (1)
  messages.append(response)

  # LLM ì‘ë‹µì— í¬í•¨ëœ ë„êµ¬ í˜¸ì¶œ ì§€ì‹œ(í•¨ìˆ˜ í˜¸ì¶œ ëª…ë ¹)ë“¤ì„ ìˆœíšŒ
  for tool_call in response.tool_calls:
      selected_tool = tool_dict[tool_call["name"]]
      tool_msg = selected_tool.invoke(tool_call) # (2)
      messages.append(tool_msg)

  # ë„êµ¬ ê²°ê³¼ê°€ í¬í•¨ëœ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ë‹¤ì‹œ LLMì— ë³´ë‚´ í›„ì† ëŒ€í™” ìƒì„±
  final_response = llm_with_tools.invoke(messages) # (3)
  ```

<details>
    <summary> ë³€ìˆ˜ê°’</summary>

```python
##### (1) #####
AIMessage(
		content='',
		additional_kwargs={
				'tool_calls': [
						{
								'id': 'call_xC0QhLkdBKHhihHG4zHFRiBX',
								'function': {
										'arguments': '{"timezone":"Asia/Seoul","location":"Busan"}',
										'name': 'get_current_time'
								},
								'type': 'function'
						}
				],
				'refusal': None
		},
		response_metadata={
				'token_usage': {
						'completion_tokens': 24,
						'prompt_tokens': 135,
						'total_tokens': 159,
						'completion_tokens_details': {
								'accepted_prediction_tokens': 0,
								'audio_tokens': 0,
								'reasoning_tokens': 0,
								'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
								'audio_tokens': 0,
								'cached_tokens': 0
						}
				},
				'model_name': 'gpt-4o-mini-2024-07-18',
				'system_fingerprint': 'fp_00428b782a',
				'finish_reason': 'tool_calls',
				'logprobs': None
		},
		id='run-eed19018-9933-4516-9241-f6c5a32a6366-0',
		tool_calls=[
				{
						'name': 'get_current_time',
						'args': {
								'timezone': 'Asia/Seoul',
								'location': 'Busan'
						},
						'id': 'call_xC0QhLkdBKHhihHG4zHFRiBX',
						'type': 'tool_call'
				}
		],
		usage_metadata={
				'input_tokens': 135,
				'output_tokens': 24,
				'total_tokens': 159,
				'input_token_details': {
						'audio': 0,
						'cache_read': 0
				},
				'output_token_details': {
						'audio': 0,
						'reasoning': 0
				}
		}
)
```

```python
##### (2) #####
ToolMessage(
		content='Asia/Seoul (Busan) í˜„ì¬ì‹œê° 2025-02-18 00:38:38 ',
		name='get_current_time',
		tool_call_id='call_xC0QhLkdBKHhihHG4zHFRiBX'
)
```

```python
##### (3) #####
AIMessage(
		content='í˜„ì¬ ë¶€ì‚°ì€ 2025ë…„ 2ì›” 18ì¼ 00ì‹œ 38ë¶„ 38ì´ˆì…ë‹ˆë‹¤.',
		additional_kwargs={'refusal': None},
		response_metadata={
				'token_usage': {
						'completion_tokens': 26,
						'prompt_tokens': 192,
						'total_tokens': 218,
						'completion_tokens_details': {
								'accepted_prediction_tokens': 0,
								'audio_tokens': 0,
								'reasoning_tokens': 0,
								'rejected_prediction_tokens': 0
						},
						'prompt_tokens_details': {
								'audio_tokens': 0,
								'cached_tokens': 0
						}
				},
				'model_name': 'gpt-4o-mini-2024-07-18',
				'system_fingerprint': 'fp_00428b782a',
				'finish_reason': 'stop',
				'logprobs': None
		},
		id='run-c66dd490-141a-4aea-8749-02a3fcbbebcb-0',
		usage_metadata={
				'input_tokens': 192,
				'output_tokens': 26,
				'total_tokens': 218,
				'input_token_details': {
						'audio': 0,
						'cache_read': 0
				},
				'output_token_details': {
						'audio': 0,
						'reasoning': 0
				}
		}
)
```

</details>

<p></p>

- íŒŒì´ë‹¨í‹±(Pydantic)ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ì˜ í˜•ì‹ì„ ëª…í™•í•˜ê²Œ ì •ì˜ ê°€ëŠ¥

  ```python
  from pydantic import BaseModel, Field

  class StockHistoryInput(BaseModel):
      ticker: str = Field(..., title="ì£¼ì‹ ì½”ë“œ", description="ì£¼ì‹ ì½”ë“œ (ì˜ˆ: AAPL)")
      period: str = Field(..., title="ê¸°ê°„", description="ì£¼ì‹ ë°ì´í„° ì¡°íšŒ ê¸°ê°„ (ì˜ˆ: 1d, 1mo, 1y)")

  @tool
  def get_yf_stock_history(stock_history_input: StockHistoryInput) -> str:
      """ ì£¼ì‹ ì¢…ëª©ì˜ ê°€ê²© ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜"""
      stock = yf.Ticker(stock_history_input.ticker)
      history = stock.history(period=stock_history_input.period)
      history_md = history.to_markdown()

      return history_md
  ```

<br />

## Stream ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” Chatbot ì˜ˆì‹œ

```python
import streamlit as st
from dotenv import load_dotenv
from datetime import datetime
import pytz

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool

# ë„êµ¬ í•¨ìˆ˜ ì •ì˜
# ì˜ˆì‹œ)
# time = get_current_time("Asia/Seoul", "ì„œìš¸")
# time = "Asia/Seoul (ì„œìš¸) í˜„ì¬ì‹œê° 2025-07-04 18:00:00"
@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
        return result
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"

# ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def get_ai_response(messages):
    response = llm_with_tools.stream(messages)

    gathered = None
    for chunk in response:
        yield chunk

        if gathered is None:
            gathered = chunk
        else:
            gathered += chunk

    if gathered.tool_calls:
        st.session_state.messages.append(gathered)

        for tool_call in gathered.tool_calls:
            selected_tool = tool_dict[tool_call['name']]
            tool_msg = selected_tool.invoke(tool_call)
            st.session_state.messages.append(tool_msg)

        for chunk in get_ai_response(st.session_state.messages):
            yield chunk

# OpenAI API Key ë“±ë¡
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini")

# ë„êµ¬ ë°”ì¸ë”©
tools = [get_current_time]
tool_dict = {"get_current_time": get_current_time}

llm_with_tools = llm.bind_tools(tools)

# Streamlit ì•±
st.title("ğŸ’¬ Chat")

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤. "),
        AIMessage("How can I help you?")
    ]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, ToolMessage):
            st.chat_message("tool").write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥

    response = get_ai_response(st.session_state["messages"])

    result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥

    st.session_state.messages.append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥

```

<details>
    <summary>ë³€ìˆ˜ê°’</summary>
    get_ai_response í•¨ìˆ˜ ì•ˆì— ìˆëŠ” chunkì™€ gatheredì˜ ë³€ìˆ˜ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŒ

<details>
    <summary>function callingì„ í•˜ì§€ ì•Šì„ ë•Œ</summary>

- messages

  ```
  messages = [
      SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤. "),
      AIMessage("How can I help you?")
      HumanMessage("ì•ˆë…•!")
  ]
  ```

- chunk

  ```python
  ##### chunk(1) #####
  {
  		"content":""
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ##### chunk(2) #####
  {
  		"content":"ì•ˆ"
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ##### chunk(3) #####
  {
  		"content":"ë…•í•˜ì„¸ìš”"
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ...

  ##### chunk(n-1) #####
  {
  		"content":"?"
  		"additional_kwargs":{}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }

  ##### chunk(n) #####
  {
  		"content":""
  		"additional_kwargs":{}
  		"response_metadata":{
  				"finish_reason":"stop"
  				"model_name":"gpt-4o-mini-2024-07-18"
  				"system_fingerprint":"fp_34a54ae93c"
  				"service_tier":"default"
  		}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }
  ```

- gathered

  ```python
    {
    		"content":"ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
    		"additional_kwargs":{}
    		"response_metadata":{
    				"finish_reason":"stop"
    				"model_name":"gpt-4o-mini-2024-07-18"
    				"system_fingerprint":"fp_34a54ae93c"
    				"service_tier":"default"
    		}
    		"type":"AIMessageChunk"
    		"name":NULL
    		"id":"run--a0eb56f1-d208-486b-a0a2-133c35d4e564"
    		"example":false
    		"tool_calls":[]
    		"invalid_tool_calls":[]
    		"usage_metadata":NULL
    		"tool_call_chunks":[]
    }
  ```

</details>

<details>
    <summary>function callingì„ í•  ë•Œ</summary>

- messages

  ```python
  messages = [
      SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤. "),
      AIMessage("How can I help you?")
      HumanMessage("ëŒ€ì „ ì‹œê°„ì€ ëª‡ì‹œì•¼?")
  ]
  ```

- chunk

  ```python
  ##### chunk(1) #####
  {
  		"content":""
  		"additional_kwargs":{
  				"tool_calls":[
  						0:{
  								"index":0
  								"id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
  								"function":{
  										"arguments":""
  										"name":"get_current_time"
  								}
  								"type":"function"
  						}
  				]
  		}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[
  				0:{
  						"name":"get_current_time"
  						"args":{}
  						"id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
  						"type":"tool_call"
  				}
  		]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[
  				0:{
  						"name":"get_current_time"
  						"args":""
  						"id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
  						"index":0
  						"type":"tool_call_chunk"
  				}
  		]
  }

  ##### chunk(2) #####
  {
  		"content":""
  		"additional_kwargs":{
  				"tool_calls":[
  						0:{
  								"index":0
  								"id":NULL
  								"function":{
  										"arguments":"{""
  										"name":NULL
  								}
  								"type":NULL
  						}
  				]
  		}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[
  				0:{
  						**"name":""**
  						"args":{}
  						**"id":NULL**
  						"type":"tool_call"
  				}
  		]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[
  				0:{
  						**"name":NULL**
  						**"args":"{""**
  						**"id":NULL**
  						"index":0
  						"type":"tool_call_chunk"
  				}
  		]
  }

  ...

  ##### chunk(n-1) #####
  {
  		"content":""
  		"additional_kwargs":{
  				"tool_calls":[
  						0:{
  								"index":0
  								"id":NULL
  								"function":{
  										"arguments":""}"
  										"name":NULL
  								}
  								"type":NULL
  						}
  				]
  		}
  		"response_metadata":{}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[
  				0:{
  						"name":NULL
  						"args":""}"
  						"id":NULL
  						"error":NULL
  						"type":"invalid_tool_call"
  				}
  		]
  		"usage_metadata":NULL
  		"tool_call_chunks":[
  				0:{
  						"name":NULL
  						"args":""}"
  						"id":NULL
  						"index":0
  						"type":"tool_call_chunk"
  				}
  		]
  }

  ##### chunk(n) #####
  {
  		"content":""
  		"additional_kwargs":{}
  		"response_metadata":{
  				"finish_reason":"tool_calls"
  				"model_name":"gpt-4o-mini-2024-07-18"
  				"system_fingerprint":"fp_34a54ae93c"
  				"service_tier":"default"
  		}
  		"type":"AIMessageChunk"
  		"name":NULL
  		"id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
  		"example":false
  		"tool_calls":[]
  		"invalid_tool_calls":[]
  		"usage_metadata":NULL
  		"tool_call_chunks":[]
  }
  ```

- gathered

  ```python
      {
              "content":""
              "additional_kwargs":{
                      "tool_calls":[
                              0:{
                                      "index":0
                                      "id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
                                      "function":{
                                              "arguments":"{"timezone":"Asia/Seoul","location":"Daejeon"}"
                                              "name":"get_current_time"
                                      }
                                      "type":"function"
                              }
                      ]
              }
              "response_metadata":{
                      "finish_reason":"tool_calls"
                      "model_name":"gpt-4o-mini-2024-07-18"
                      "system_fingerprint":"fp_34a54ae93c"
                      "service_tier":"default"
              }
              "type":"AIMessageChunk"
              "name":NULL
              "id":"run--5e9ba21c-e6d9-4f65-ba42-878533c50fc8"
              "example":false
              "tool_calls":[
                      0:{
                              "name":"get_current_time"
                              "args":{
                                      "timezone":"Asia/Seoul"
                                      "location":"Daejeon"
                              }
                              "id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
                              "type":"tool_call"
                      }
              ]
              "invalid_tool_calls":[]
              "usage_metadata":NULL
              "tool_call_chunks":[
                      0:{
                              "name":"get_current_time"
                              "args":"{"timezone":"Asia/Seoul","location":"Daejeon"}"
                              "id":"call_dRnE7tvXt47zgV7kiN7sdXYy"
                              "index":0
                              "type":"tool_call_chunk"
                      }
              ]
      }
  ```

</details>
</details>

<br />
<br />
<br />

[ì¶œì²˜: ì´ì„±ìš©, ã€ŒDo it! LLMì„ í™œìš©í•œ AI ì—ì´ì „íŠ¸ ê°œë°œ ì…ë¬¸ - GPT API+ë”¥ì‹œí¬+ë¼ë§ˆ+ë­ì²´ì¸+ë­ê·¸ë˜í”„+RAGã€, ì´ì§€ìŠ¤í¼ë¸”ë¦¬ì‹±](https://www.easyspub.co.kr/20_Menu/BookView/764/PUB)

<br />
