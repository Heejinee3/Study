<br />

## RAG(Retrieval-Augmented Generation)란?

문서를 검색해서(Chat Retrieval), 그 내용을 바탕으로 LLM이 대답하게 만드는 방식

<br />

---

### Context Window

- LLM이 한 번에 이해할 수 있는 최대 입력 토큰 범위
- LLM은 대화를 이어가거나 문서를 이해할 때, 과거 입력(문장, 문맥)을 모두 기억하지 못함
- 대신 최근 입력된 토큰만 기억하며 처리하는데, 이 기억할 수 있는 범위가 바로 Context Window임

<br />

---

### Embedding

단어/문장/문서 같은 텍스트를 숫자 벡터로 바꾸는 것

```python
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
vector = embedding_model.embed_query("서울시의 탄소 감축 정책")

print(vector) # [0.0342, -0.192, 0.874, ..., 0.0021]
```

<br />

---

### Vector DB

임베딩된 벡터(숫자 배열)를 저장하고, 비슷한 벡터를 빠르게 찾아주는 DB

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# 1. 임베딩 모델 정의
embedding = OpenAIEmbeddings(model="text-embedding-3-large")

# 2. 문서 임베딩 후 벡터 DB 생성
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embedding,
    persist_directory="./chroma_db"
)
```

| 대표 Vector DB  | 특징                                           |
| --------------- | ---------------------------------------------- |
| 🔵 **Chroma**   | 가볍고 로컬에서도 잘 작동, LangChain과 잘 연동 |
| 🟢 **FAISS**    | Facebook 오픈소스, 빠름, GPU도 지원            |
| 🟣 **Pinecone** | 클라우드 기반, 대규모 실시간 검색              |
| 🔴 **Weaviate** | 메타데이터 필터링, 그래프 구조도 지원          |
| 🟡 **Qdrant**   | Rust 기반, 성능 좋고 오픈소스                  |

<br />

---

### Retriever

사용자의 질문을 임베딩해서, Vector DB에서 관련 문서를 찾아주는 객체

```python
retriever = vectorstore.as_retriever(k=3)
docs = retriever.invoke("서울시의 탄소 저감 정책은?")
```

```python
docs = [
            Document(
                id='98595fe0-5893-4316-ba65-8fe6765668f4',
                metadata={
                    'page': 85,
                    'author': 'SI',
                    'source': './data/2040_seoul_plan.pdf',
                    'pdfversion': '1.4',
                    'total_pages': 205,
                    'producer': 'Hancom PDF 1.3.0.542',
                    'creationdate': '2024-12-12T18:16:11+09:00',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page_label': '86',
                    'creator': 'Hwp 2020 11.0.0.5178'
                },
                page_content='제4절 기후·환경 부문1. 개요Ÿ기후변화는 21세기에 전 지구적으로 가장 위중한 영향을 미칠 것으로 예상되며, 시민 생활의 모든 ...'
            ),
            Document(
                id='160cbd6a-436c-45e6-9e2e-c6892c1a569d',
                metadata={
                    'total_pages': 205,
                    'author': 'SI',
                    'creator': 'Hwp 2020 11.0.0.5178',
                    'pdfversion': '1.4',
                    'page_label': '89',
                    'source': './data/2040_seoul_plan.pdf',
                    'creationdate': '2024-12-12T18:16:11+09:00',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page': 88,
                    'producer': 'Hancom PDF 1.3.0.542'
                },
                page_content='제4절 기후·환경 부문813-2-2 기후 행동 포용적 거버넌스 구축을 위한 시민 행동 활성화Ÿ자원순환부터  기후 행동까지 수도권 시민과 정부, ...'
            ),
            Document(
                id='7be9ab52-b110-4429-b25f-0ab42319c0b2',
                metadata={
                    'source': './data/2040_seoul_plan.pdf',
                    'creationdate': '2024-12-12T18:16:11+09:00',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page': 87,
                    'producer': 'Hancom PDF 1.3.0.542',
                    'pdfversion': '1.4',
                    'total_pages': 205,
                    'creator': 'Hwp 2020 11.0.0.5178',
                    'author': 'SI',
                    'page_label': '88'
                },
                page_content='Ÿ소규모 분산형 발전 시설 확대를 위한 대체에너지 설치 지원을 지속적으로 확대하고, 공공시설의 옥상 및 주차장 등의 공간을 활용하여 ...'
            ),
            Document(
                id='4e672987-6446-4fa4-9175-6d4da5962800',
                metadata={
                    'pdfversion': '1.4',
                    'total_pages': 205,
                    'page': 85,
                    'author': 'SI',
                    'creationdate': '2024-12-12T18:16:11+09:00', 'creator': 'Hwp 2020 11.0.0.5178',
                    'source': './data/2040_seoul_plan.pdf',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page_label': '86',
                    'producer': 'Hancom PDF 1.3.0.542'
                },
                page_content='78제3장 부문별 전 략계획\n제4절 기후·환경 부문1. 개요Ÿ기후변화는 21세기에 전 지구적으로 가장 위중한 영향을 미칠 것으로 예상되며, ...'
            )
        ]
```

<br />

---

### Query Augmentation

사용자의 질문을 더 명확하거나 풍부하게 보완/재작성해서 검색 성능을 높이는 기법

```python
# 예시

[이전 대화]
Human: 서울시의 에너지 정책 알려줘
AI: 건물 에너지 효율화, 신재생 확대 등을 포함합니다.

[현재 질문]
Human: 그럼 그건 얼마나 효과 있어?

[Query Aug 결과]
→ "서울시의 건물 에너지 효율화 정책은 얼마나 효과가 있었는가?"
```

```python
# 코드

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 프롬프트 정의
query_augmentation_prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="messages"),  # 이전 대화
    ("system", "사용자의 아래 질문을 명확한 한 문장으로 재작성하세요. 대명사 대신 구체적인 명사로 바꾸세요.:\n\n{query}")
])

# 체인 생성
query_augmentation_chain = query_augmentation_prompt | ChatOpenAI() | StrOutputParser()

# 사용자의 모호한 질문
user_query = "그 정책은 얼마나 잘 되고 있어?"

# 이전 대화 포함
context = [
  HumanMessage(content="서울시의 탄소중립 계획 알려줘"),
  AIMessage(content="2030년까지 배출량 40% 감축을 목표로 합니다."),
]

# augmentation 실행
clarified_query = query_augmentation_chain.invoke({
    "messages": context,
    "query": user_query
})

print(clarified_query) # "서울시의 2030년 탄소중립 목표 달성 현황은 어떠한가요?"
```

<br />

## RAG 기반 문서 질의응답 시스템

### 설치

```python
pip install langchain langchain_community langchain_chroma langchain_openai
pip install PyMuPDF pypdf
pip install streamlit
pip install python-dotenv
```

<br />

### Console/Script Version

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.memory import ChatMessageHistory
from langchain_chroma import Chroma

from dotenv import load_dotenv
import os

########################################################################
###################### Step1. 벡터 DB 생성 및 검색 ######################
########################################################################

# OPENAI_API_KEY 로드
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# 임베딩 모델로 text-embedding-3-large 사용
embedding = OpenAIEmbeddings(model='text-embedding-3-large', api_key=OPENAI_API_KEY)

persist_directory = './chroma_store'

# 저장된 크로마 DB가 없다면 새로운 Chroma 인스턴스를 생성하고, 문서를 벡터화하여 저장
if not os.path.exists(persist_directory):
    print("Creating new Chroma store")

    # 텍스트 데이터를 1000자 단위로 나누고 overlap을 100자로 설정
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    # OneNYC_2050_Strategic_Plan
    loader_nyc = PyPDFLoader('./data/OneNYC_2050_Strategic_Plan.pdf')
    data_nyc = loader_nyc.load()
    all_splits = text_splitter.split_documents(data_nyc)

    # 2040_seoul_plan
    loader_seoul = PyPDFLoader('./data/2040_seoul_plan.pdf')
    data_seoul = loader_seoul.load()
    seoul_splits = text_splitter.split_documents(data_seoul)

    """
    2040_seoul_plan의 경우,
    한 페이지에 글자수가 1000을 넘어가지 못하는 경우가 많아
    한 청크가 한 페이지가 되는 경우가 많음 (overlap도 없음)
    따라서, 직접 overlap을 구현
    """
    for i in range(len(seoul_splits) - 1):
        seoul_splits[i].page_content += "\n"+ seoul_splits[i + 1].page_content[:100]

    # 뉴욕과 서울 문서 청크를 합침
    all_splits.extend(seoul_splits)

    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=embedding,
        persist_directory=persist_directory
    )

# 이미 저장된 벡터스토어를 로드하여 사용
else:
    print("Loading existing Chroma store")
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )

# Retriever: 질문(query)을 벡터로 바꿔서 벡터 DB(예: Chroma)에서 유사한 문서들을 찾아주는 객체
retriever = vectorstore.as_retriever(k=3)
docs = retriever.invoke("서울시의 환경 정책에 대해 궁금해")

########################################################################
################### Step2. 질의응답 체인 구성 및 실행  ###################
########################################################################

model = ChatOpenAI(model="gpt-4o-mini")

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "사용자의 질문에 대해 아래 context에 기반하여 답변하라.:\n\n{context}",
        ),
        # 프롬프트 내부에 대화 이력(messages)을 삽입할 자리를 지정
        MessagesPlaceholder(variable_name="messages"),
    ]
)

# 여러 문서를 stuffing 방식으로 넣어서 처리하는 체인
chain = create_stuff_documents_chain(model, prompt)

"""
ChatMessageHistory
 └── messages: List[BaseMessage]
       ├── HumanMessage(content="...", additional_kwargs={})
       ├── AIMessage(content="...", additional_kwargs={})
       └── SystemMessage(content="...", additional_kwargs={})
"""
history = ChatMessageHistory()
history.add_user_message("서울시의 온실가스 저감 정책에 대해 알려줘.")

answer = chain.invoke(
    {
        "messages": history.messages,
        "context": docs,
    }
)

history.add_ai_message(answer)
print(answer)
```

<br />

### Streamlit Version

```python
# gpt.py

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
import retriever

# 모델 초기화
llm = ChatOpenAI(model="gpt-4o-mini")

# 사용자의 메시지 처리하기 위한 함수
def get_ai_response(messages, docs):
    response = retriever.document_chain.stream({
        "messages": messages,
        "context": docs
    })

    for chunk in response:
        yield chunk

# Streamlit 앱
st.title("💬 GPT-4o Langchain Chat")

# 스트림릿 session_state에 메시지 저장
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("너는 문서에 기반해 답변하는 도시 정책 전문가야 "),
        AIMessage("How can I help you?")
    ]

# 스트림릿 화면에 메시지 출력
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)

# 사용자 입력 처리
if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    st.session_state.messages.append(HumanMessage(prompt))

    # 질의 확장
    augmented_query = retriever.query_augmentation_chain.invoke({
        "messages": st.session_state["messages"],
        "query": prompt,
    })
    print("augmented_query: ", augmented_query)

    # 관련 문서 검색
    docs = retriever.retriever.invoke(f"{prompt}\n{augmented_query}")

    # 파일명과 페이지 정보 표시
    for doc in docs:
        with st.expander(f"**문서:** {doc.metadata.get('source', '알 수 없음')}"):
            st.write(f"page: {doc.metadata.get('page', '')}")
            st.write(doc.page_content)

    # AI 메시지 출력
    with st.spinner(f"AI가 답변을 준비 중입니다... '{augmented_query}'"):
        response = get_ai_response(st.session_state["messages"], docs)
        result = st.chat_message("assistant").write_stream(response)

    # AI 메시지 저장
    st.session_state["messages"].append(AIMessage(result))
```

```python
# retriever.py

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.output_parsers import StrOutputParser

from dotenv import load_dotenv
import os

#######################################################
################# 벡터 DB 생성 및 검색 #################
#######################################################

# OPENAI_API_KEY 로드
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# 임베딩 모델로 text-embedding-3-large 사용
embedding = OpenAIEmbeddings(model='text-embedding-3-large', api_key=OPENAI_API_KEY)

persist_directory = './chroma_store'

# 저장된 크로마 DB가 없다면 새로운 Chroma 인스턴스를 생성하고, 문서를 벡터화하여 저장
if not os.path.exists(persist_directory):
    print("Creating new Chroma store")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    loader_nyc = PyPDFLoader('./data/OneNYC_2050_Strategic_Plan.pdf')
    data_nyc = loader_nyc.load()
    all_splits = text_splitter.split_documents(data_nyc)

    loader_seoul = PyPDFLoader('./data/2040_seoul_plan.pdf')
    data_seoul = loader_seoul.load()
    seoul_splits = text_splitter.split_documents(data_seoul)

    for i in range(len(seoul_splits) - 1):
        seoul_splits[i].page_content += "\n"+ seoul_splits[i + 1].page_content[:100]

    all_splits.extend(seoul_splits)

    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=embedding,
        persist_directory=persist_directory
    )

# 이미 저장된 벡터스토어를 로드하여 사용
else:
    print("Loading existing Chroma store")
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )

# retriever
retriever = vectorstore.as_retriever(k=3)

#######################################################
############### 문서 기반 QA chain 정의 ################
#######################################################

# 언어 모델 불러오기
llm = ChatOpenAI(model="gpt-4o")

# document chain
question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "사용자의 질문에 대해 아래 context에 기반하여 답변하라.:\n\n{context}",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(llm, question_answering_prompt) | StrOutputParser()

#######################################################
################# 질의 확장 chain 정의 #################
#######################################################

# query augmentation chain
query_augmentation_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"), # 기존 대화 내용
        (
            "system",
            "기존의 대화 내용을 활용하여 사용자의 아래 질문의 의도를 파악하여 명료한 한 문장의 질문으로 변환하라. 대명사나 이, 저, 그와 같은 표현을 명확한 명사로 표현하라. :\n\n{query}",
        ),
    ]
)

query_augmentation_chain = query_augmentation_prompt | llm | StrOutputParser()
```

<br />
<br />
<br />

[출처: 이성용, 「Do it! LLM을 활용한 AI 에이전트 개발 입문 - GPT API+딥시크+라마+랭체인+랭그래프+RAG」, 이지스퍼블리싱](https://www.easyspub.co.kr/20_Menu/BookView/764/PUB)

<br />
