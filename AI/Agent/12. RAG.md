<br />

## RAG(Retrieval-Augmented Generation)ë€?

ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì„œ(Chat Retrieval), ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ LLMì´ ëŒ€ë‹µí•˜ê²Œ ë§Œë“œëŠ” ë°©ì‹

<br />

---

### Context Window

- LLMì´ í•œ ë²ˆì— ì´í•´í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì…ë ¥ í† í° ë²”ìœ„
- LLMì€ ëŒ€í™”ë¥¼ ì´ì–´ê°€ê±°ë‚˜ ë¬¸ì„œë¥¼ ì´í•´í•  ë•Œ, ê³¼ê±° ì…ë ¥(ë¬¸ì¥, ë¬¸ë§¥)ì„ ëª¨ë‘ ê¸°ì–µí•˜ì§€ ëª»í•¨
- ëŒ€ì‹  ìµœê·¼ ì…ë ¥ëœ í† í°ë§Œ ê¸°ì–µí•˜ë©° ì²˜ë¦¬í•˜ëŠ”ë°, ì´ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” ë²”ìœ„ê°€ ë°”ë¡œ Context Windowì„

<br />

---

### Embedding

ë‹¨ì–´/ë¬¸ì¥/ë¬¸ì„œ ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” ê²ƒ

```python
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
vector = embedding_model.embed_query("ì„œìš¸ì‹œì˜ íƒ„ì†Œ ê°ì¶• ì •ì±…")

print(vector) # [0.0342, -0.192, 0.874, ..., 0.0021]
```

<br />

---

### Vector DB

ì„ë² ë”©ëœ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¥¼ ì €ì¥í•˜ê³ , ë¹„ìŠ·í•œ ë²¡í„°ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì•„ì£¼ëŠ” DB

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# 1. ì„ë² ë”© ëª¨ë¸ ì •ì˜
embedding = OpenAIEmbeddings(model="text-embedding-3-large")

# 2. ë¬¸ì„œ ì„ë² ë”© í›„ ë²¡í„° DB ìƒì„±
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embedding,
    persist_directory="./chroma_db"
)
```

| ëŒ€í‘œ Vector DB  | íŠ¹ì§•                                           |
| --------------- | ---------------------------------------------- |
| ğŸ”µ **Chroma**   | ê°€ë³ê³  ë¡œì»¬ì—ì„œë„ ì˜ ì‘ë™, LangChainê³¼ ì˜ ì—°ë™ |
| ğŸŸ¢ **FAISS**    | Facebook ì˜¤í”ˆì†ŒìŠ¤, ë¹ ë¦„, GPUë„ ì§€ì›            |
| ğŸŸ£ **Pinecone** | í´ë¼ìš°ë“œ ê¸°ë°˜, ëŒ€ê·œëª¨ ì‹¤ì‹œê°„ ê²€ìƒ‰              |
| ğŸ”´ **Weaviate** | ë©”íƒ€ë°ì´í„° í•„í„°ë§, ê·¸ë˜í”„ êµ¬ì¡°ë„ ì§€ì›          |
| ğŸŸ¡ **Qdrant**   | Rust ê¸°ë°˜, ì„±ëŠ¥ ì¢‹ê³  ì˜¤í”ˆì†ŒìŠ¤                  |

<br />

---

### Retriever

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©í•´ì„œ, Vector DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ì£¼ëŠ” ê°ì²´

```python
retriever = vectorstore.as_retriever(k=3)
docs = retriever.invoke("ì„œìš¸ì‹œì˜ íƒ„ì†Œ ì €ê° ì •ì±…ì€?")
```

```python
docs = [
            Document(
                id='98595fe0-5893-4316-ba65-8fe6765668f4',
                metadata={
                    'page': 85,
                    'author': 'SI',
                    'source': './data/2040_seoul_plan.pdf',
                    'pdfversion': '1.4',
                    'total_pages': 205,
                    'producer': 'Hancom PDF 1.3.0.542',
                    'creationdate': '2024-12-12T18:16:11+09:00',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page_label': '86',
                    'creator': 'Hwp 2020 11.0.0.5178'
                },
                page_content='ì œ4ì ˆ ê¸°í›„Â·í™˜ê²½ ë¶€ë¬¸1. ê°œìš”Å¸ê¸°í›„ë³€í™”ëŠ” 21ì„¸ê¸°ì— ì „ ì§€êµ¬ì ìœ¼ë¡œ ê°€ì¥ ìœ„ì¤‘í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ì‹œë¯¼ ìƒí™œì˜ ëª¨ë“  ...'
            ),
            Document(
                id='160cbd6a-436c-45e6-9e2e-c6892c1a569d',
                metadata={
                    'total_pages': 205,
                    'author': 'SI',
                    'creator': 'Hwp 2020 11.0.0.5178',
                    'pdfversion': '1.4',
                    'page_label': '89',
                    'source': './data/2040_seoul_plan.pdf',
                    'creationdate': '2024-12-12T18:16:11+09:00',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page': 88,
                    'producer': 'Hancom PDF 1.3.0.542'
                },
                page_content='ì œ4ì ˆ ê¸°í›„Â·í™˜ê²½ ë¶€ë¬¸813-2-2 ê¸°í›„ í–‰ë™ í¬ìš©ì  ê±°ë²„ë„ŒìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ ì‹œë¯¼ í–‰ë™ í™œì„±í™”Å¸ìì›ìˆœí™˜ë¶€í„°  ê¸°í›„ í–‰ë™ê¹Œì§€ ìˆ˜ë„ê¶Œ ì‹œë¯¼ê³¼ ì •ë¶€, ...'
            ),
            Document(
                id='7be9ab52-b110-4429-b25f-0ab42319c0b2',
                metadata={
                    'source': './data/2040_seoul_plan.pdf',
                    'creationdate': '2024-12-12T18:16:11+09:00',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page': 87,
                    'producer': 'Hancom PDF 1.3.0.542',
                    'pdfversion': '1.4',
                    'total_pages': 205,
                    'creator': 'Hwp 2020 11.0.0.5178',
                    'author': 'SI',
                    'page_label': '88'
                },
                page_content='Å¸ì†Œê·œëª¨ ë¶„ì‚°í˜• ë°œì „ ì‹œì„¤ í™•ëŒ€ë¥¼ ìœ„í•œ ëŒ€ì²´ì—ë„ˆì§€ ì„¤ì¹˜ ì§€ì›ì„ ì§€ì†ì ìœ¼ë¡œ í™•ëŒ€í•˜ê³ , ê³µê³µì‹œì„¤ì˜ ì˜¥ìƒ ë° ì£¼ì°¨ì¥ ë“±ì˜ ê³µê°„ì„ í™œìš©í•˜ì—¬ ...'
            ),
            Document(
                id='4e672987-6446-4fa4-9175-6d4da5962800',
                metadata={
                    'pdfversion': '1.4',
                    'total_pages': 205,
                    'page': 85,
                    'author': 'SI',
                    'creationdate': '2024-12-12T18:16:11+09:00', 'creator': 'Hwp 2020 11.0.0.5178',
                    'source': './data/2040_seoul_plan.pdf',
                    'moddate': '2024-12-12T18:16:11+09:00',
                    'page_label': '86',
                    'producer': 'Hancom PDF 1.3.0.542'
                },
                page_content='78ì œ3ì¥ ë¶€ë¬¸ë³„ ì „ ëµê³„íš\nì œ4ì ˆ ê¸°í›„Â·í™˜ê²½ ë¶€ë¬¸1. ê°œìš”Å¸ê¸°í›„ë³€í™”ëŠ” 21ì„¸ê¸°ì— ì „ ì§€êµ¬ì ìœ¼ë¡œ ê°€ì¥ ìœ„ì¤‘í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ...'
            )
        ]
```

<br />

---

### Query Augmentation

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê±°ë‚˜ í’ë¶€í•˜ê²Œ ë³´ì™„/ì¬ì‘ì„±í•´ì„œ ê²€ìƒ‰ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê¸°ë²•

```python
# ì˜ˆì‹œ

[ì´ì „ ëŒ€í™”]
Human: ì„œìš¸ì‹œì˜ ì—ë„ˆì§€ ì •ì±… ì•Œë ¤ì¤˜
AI: ê±´ë¬¼ ì—ë„ˆì§€ íš¨ìœ¨í™”, ì‹ ì¬ìƒ í™•ëŒ€ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.

[í˜„ì¬ ì§ˆë¬¸]
Human: ê·¸ëŸ¼ ê·¸ê±´ ì–¼ë§ˆë‚˜ íš¨ê³¼ ìˆì–´?

[Query Aug ê²°ê³¼]
â†’ "ì„œìš¸ì‹œì˜ ê±´ë¬¼ ì—ë„ˆì§€ íš¨ìœ¨í™” ì •ì±…ì€ ì–¼ë§ˆë‚˜ íš¨ê³¼ê°€ ìˆì—ˆëŠ”ê°€?"
```

```python
# ì½”ë“œ

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# í”„ë¡¬í”„íŠ¸ ì •ì˜
query_augmentation_prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="messages"),  # ì´ì „ ëŒ€í™”
    ("system", "ì‚¬ìš©ìì˜ ì•„ë˜ ì§ˆë¬¸ì„ ëª…í™•í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”. ëŒ€ëª…ì‚¬ ëŒ€ì‹  êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ ë°”ê¾¸ì„¸ìš”.:\n\n{query}")
])

# ì²´ì¸ ìƒì„±
query_augmentation_chain = query_augmentation_prompt | ChatOpenAI() | StrOutputParser()

# ì‚¬ìš©ìì˜ ëª¨í˜¸í•œ ì§ˆë¬¸
user_query = "ê·¸ ì •ì±…ì€ ì–¼ë§ˆë‚˜ ì˜ ë˜ê³  ìˆì–´?"

# ì´ì „ ëŒ€í™” í¬í•¨
context = [
  HumanMessage(content="ì„œìš¸ì‹œì˜ íƒ„ì†Œì¤‘ë¦½ ê³„íš ì•Œë ¤ì¤˜"),
  AIMessage(content="2030ë…„ê¹Œì§€ ë°°ì¶œëŸ‰ 40% ê°ì¶•ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤."),
]

# augmentation ì‹¤í–‰
clarified_query = query_augmentation_chain.invoke({
    "messages": context,
    "query": user_query
})

print(clarified_query) # "ì„œìš¸ì‹œì˜ 2030ë…„ íƒ„ì†Œì¤‘ë¦½ ëª©í‘œ ë‹¬ì„± í˜„í™©ì€ ì–´ë– í•œê°€ìš”?"
```

<br />

## RAG ê¸°ë°˜ ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

### ì„¤ì¹˜

```python
pip install langchain langchain_community langchain_chroma langchain_openai
pip install PyMuPDF pypdf
pip install streamlit
pip install python-dotenv
```

<br />

### Console/Script Version

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.memory import ChatMessageHistory
from langchain_chroma import Chroma

from dotenv import load_dotenv
import os

########################################################################
###################### Step1. ë²¡í„° DB ìƒì„± ë° ê²€ìƒ‰ ######################
########################################################################

# OPENAI_API_KEY ë¡œë“œ
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# ì„ë² ë”© ëª¨ë¸ë¡œ text-embedding-3-large ì‚¬ìš©
embedding = OpenAIEmbeddings(model='text-embedding-3-large', api_key=OPENAI_API_KEY)

persist_directory = './chroma_store'

# ì €ì¥ëœ í¬ë¡œë§ˆ DBê°€ ì—†ë‹¤ë©´ ìƒˆë¡œìš´ Chroma ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
if not os.path.exists(persist_directory):
    print("Creating new Chroma store")

    # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ 1000ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  overlapì„ 100ìë¡œ ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    # OneNYC_2050_Strategic_Plan
    loader_nyc = PyPDFLoader('./data/OneNYC_2050_Strategic_Plan.pdf')
    data_nyc = loader_nyc.load()
    all_splits = text_splitter.split_documents(data_nyc)

    # 2040_seoul_plan
    loader_seoul = PyPDFLoader('./data/2040_seoul_plan.pdf')
    data_seoul = loader_seoul.load()
    seoul_splits = text_splitter.split_documents(data_seoul)

    """
    2040_seoul_planì˜ ê²½ìš°,
    í•œ í˜ì´ì§€ì— ê¸€ììˆ˜ê°€ 1000ì„ ë„˜ì–´ê°€ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„
    í•œ ì²­í¬ê°€ í•œ í˜ì´ì§€ê°€ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ (overlapë„ ì—†ìŒ)
    ë”°ë¼ì„œ, ì§ì ‘ overlapì„ êµ¬í˜„
    """
    for i in range(len(seoul_splits) - 1):
        seoul_splits[i].page_content += "\n"+ seoul_splits[i + 1].page_content[:100]

    # ë‰´ìš•ê³¼ ì„œìš¸ ë¬¸ì„œ ì²­í¬ë¥¼ í•©ì¹¨
    all_splits.extend(seoul_splits)

    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=embedding,
        persist_directory=persist_directory
    )

# ì´ë¯¸ ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
else:
    print("Loading existing Chroma store")
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )

# Retriever: ì§ˆë¬¸(query)ì„ ë²¡í„°ë¡œ ë°”ê¿”ì„œ ë²¡í„° DB(ì˜ˆ: Chroma)ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì°¾ì•„ì£¼ëŠ” ê°ì²´
retriever = vectorstore.as_retriever(k=3)
docs = retriever.invoke("ì„œìš¸ì‹œì˜ í™˜ê²½ ì •ì±…ì— ëŒ€í•´ ê¶ê¸ˆí•´")

########################################################################
################### Step2. ì§ˆì˜ì‘ë‹µ ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰  ###################
########################################################################

model = ChatOpenAI(model="gpt-4o-mini")

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼.:\n\n{context}",
        ),
        # í”„ë¡¬í”„íŠ¸ ë‚´ë¶€ì— ëŒ€í™” ì´ë ¥(messages)ì„ ì‚½ì…í•  ìë¦¬ë¥¼ ì§€ì •
        MessagesPlaceholder(variable_name="messages"),
    ]
)

# ì—¬ëŸ¬ ë¬¸ì„œë¥¼ stuffing ë°©ì‹ìœ¼ë¡œ ë„£ì–´ì„œ ì²˜ë¦¬í•˜ëŠ” ì²´ì¸
chain = create_stuff_documents_chain(model, prompt)

"""
ChatMessageHistory
 â””â”€â”€ messages: List[BaseMessage]
       â”œâ”€â”€ HumanMessage(content="...", additional_kwargs={})
       â”œâ”€â”€ AIMessage(content="...", additional_kwargs={})
       â””â”€â”€ SystemMessage(content="...", additional_kwargs={})
"""
history = ChatMessageHistory()
history.add_user_message("ì„œìš¸ì‹œì˜ ì˜¨ì‹¤ê°€ìŠ¤ ì €ê° ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì¤˜.")

answer = chain.invoke(
    {
        "messages": history.messages,
        "context": docs,
    }
)

history.add_ai_message(answer)
print(answer)
```

<br />

### Streamlit Version

```python
# gpt.py

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
import retriever

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini")

# ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def get_ai_response(messages, docs):
    response = retriever.document_chain.stream({
        "messages": messages,
        "context": docs
    })

    for chunk in response:
        yield chunk

# Streamlit ì•±
st.title("ğŸ’¬ GPT-4o Langchain Chat")

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ë„ì‹œ ì •ì±… ì „ë¬¸ê°€ì•¼ "),
        AIMessage("How can I help you?")
    ]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    st.session_state.messages.append(HumanMessage(prompt))

    # ì§ˆì˜ í™•ì¥
    augmented_query = retriever.query_augmentation_chain.invoke({
        "messages": st.session_state["messages"],
        "query": prompt,
    })
    print("augmented_query: ", augmented_query)

    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.retriever.invoke(f"{prompt}\n{augmented_query}")

    # íŒŒì¼ëª…ê³¼ í˜ì´ì§€ ì •ë³´ í‘œì‹œ
    for doc in docs:
        with st.expander(f"**ë¬¸ì„œ:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}"):
            st.write(f"page: {doc.metadata.get('page', '')}")
            st.write(doc.page_content)

    # AI ë©”ì‹œì§€ ì¶œë ¥
    with st.spinner(f"AIê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... '{augmented_query}'"):
        response = get_ai_response(st.session_state["messages"], docs)
        result = st.chat_message("assistant").write_stream(response)

    # AI ë©”ì‹œì§€ ì €ì¥
    st.session_state["messages"].append(AIMessage(result))
```

```python
# retriever.py

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.output_parsers import StrOutputParser

from dotenv import load_dotenv
import os

#######################################################
################# ë²¡í„° DB ìƒì„± ë° ê²€ìƒ‰ #################
#######################################################

# OPENAI_API_KEY ë¡œë“œ
env_path = "api_key.env"
load_dotenv(dotenv_path=env_path)
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# ì„ë² ë”© ëª¨ë¸ë¡œ text-embedding-3-large ì‚¬ìš©
embedding = OpenAIEmbeddings(model='text-embedding-3-large', api_key=OPENAI_API_KEY)

persist_directory = './chroma_store'

# ì €ì¥ëœ í¬ë¡œë§ˆ DBê°€ ì—†ë‹¤ë©´ ìƒˆë¡œìš´ Chroma ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
if not os.path.exists(persist_directory):
    print("Creating new Chroma store")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    loader_nyc = PyPDFLoader('./data/OneNYC_2050_Strategic_Plan.pdf')
    data_nyc = loader_nyc.load()
    all_splits = text_splitter.split_documents(data_nyc)

    loader_seoul = PyPDFLoader('./data/2040_seoul_plan.pdf')
    data_seoul = loader_seoul.load()
    seoul_splits = text_splitter.split_documents(data_seoul)

    for i in range(len(seoul_splits) - 1):
        seoul_splits[i].page_content += "\n"+ seoul_splits[i + 1].page_content[:100]

    all_splits.extend(seoul_splits)

    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=embedding,
        persist_directory=persist_directory
    )

# ì´ë¯¸ ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
else:
    print("Loading existing Chroma store")
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )

# retriever
retriever = vectorstore.as_retriever(k=3)

#######################################################
############### ë¬¸ì„œ ê¸°ë°˜ QA chain ì •ì˜ ################
#######################################################

# ì–¸ì–´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
llm = ChatOpenAI(model="gpt-4o")

# document chain
question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼.:\n\n{context}",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(llm, question_answering_prompt) | StrOutputParser()

#######################################################
################# ì§ˆì˜ í™•ì¥ chain ì •ì˜ #################
#######################################################

# query augmentation chain
query_augmentation_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"), # ê¸°ì¡´ ëŒ€í™” ë‚´ìš©
        (
            "system",
            "ê¸°ì¡´ì˜ ëŒ€í™” ë‚´ìš©ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì•„ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ëª…ë£Œí•œ í•œ ë¬¸ì¥ì˜ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ë¼. ëŒ€ëª…ì‚¬ë‚˜ ì´, ì €, ê·¸ì™€ ê°™ì€ í‘œí˜„ì„ ëª…í™•í•œ ëª…ì‚¬ë¡œ í‘œí˜„í•˜ë¼. :\n\n{query}",
        ),
    ]
)

query_augmentation_chain = query_augmentation_prompt | llm | StrOutputParser()
```

<br />
<br />
<br />

[ì¶œì²˜: ì´ì„±ìš©, ã€ŒDo it! LLMì„ í™œìš©í•œ AI ì—ì´ì „íŠ¸ ê°œë°œ ì…ë¬¸ - GPT API+ë”¥ì‹œí¬+ë¼ë§ˆ+ë­ì²´ì¸+ë­ê·¸ë˜í”„+RAGã€, ì´ì§€ìŠ¤í¼ë¸”ë¦¬ì‹±](https://www.easyspub.co.kr/20_Menu/BookView/764/PUB)

<br />
