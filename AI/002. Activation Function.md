<br />

## 활성화 함수 (Activation Function)

은닉층(Hidden Layer)과 출력층(Output Layer)의 뉴런에서 출력값을 결정하는 함수

비선형 함수(Nonlinear function)여야 함

<p></p>

## 계단 함수 (Step Function)

$$
H(x) = \begin{cases} 0 & \text{if } x < 0 \\1 & \text{if } x \geq 0\end{cases}
$$

<div style="display: flex; justify-content: center;">
  <img src="https://raw.githubusercontent.com/Heejinee3/Study/refs/heads/master/AI/Figure/Figure4.png" style="margin: 0 10px; width: 300px;">
</div>

<p></p>

## 하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent)

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

<div style="display: flex; justify-content: center;">
  <img src="https://raw.githubusercontent.com/Heejinee3/Study/refs/heads/master/AI/Figure/Figure5.png" style="margin: 0 10px; width: 300px;">
</div>

## 시그모이드 함수 (Sigmoid Function)

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

<div style="display: flex; justify-content: center;">
  <img src="https://raw.githubusercontent.com/Heejinee3/Study/refs/heads/master/AI/Figure/Figur6.png" style="margin: 0 10px; width: 300px;">
</div>

## Softmax 함수 (Softmax Function)

$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} = \frac{e^{x_i - \max(x)}}{\sum_{j} e^{x_j - \max(x)}}
$$

<div style="display: flex; justify-content: center;">
  <img src="https://raw.githubusercontent.com/Heejinee3/Study/refs/heads/master/AI/Figure/Figur7.png" style="margin: 0 10px; width: 300px;">
</div>

<p></p>

## ReLU 함수 (Rectified Linear Unit Function)

$$
\text{ReLU}(x) = \max(0, x)
$$

<div style="display: flex; justify-content: center;">
  <img src="https://raw.githubusercontent.com/Heejinee3/Study/refs/heads/master/AI/Figure/Figur8.png" style="margin: 0 10px; width: 300px;">
</div>

<p></p>

## Leaky ReLU 함수

$$
\text{Leaky ReLU}(x) = \begin{cases}       x & x > 0 \\      0.01x & x \leq 0    \end{cases}
$$

<div style="display: flex; justify-content: center;">
  <img src="https://raw.githubusercontent.com/Heejinee3/Study/refs/heads/master/AI/Figure/Figur9.png" style="margin: 0 10px; width: 300px;">
</div>

<p></p>

$ \text{softmax}(x*i) = \frac{e^{x_i}}{\sum*{j} e^{x_j}} $
